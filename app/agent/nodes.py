"""
ë…¸ë“œ í•¨ìˆ˜ (Self-RAG êµ¬ì¡°)
"""
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AIMessage
from app.agent.state import AgentState
from app.agent.prompts import (
    DOC_RELEVANCE_PROMPT_TEMPLATE, 
    RESPONSE_GENERATION_PROMPT_TEMPLATE,
    AGENT_NODE_PROMPT_TEMPLATE,
    get_baby_context_string,
    get_docs_context_string,
    EMERGENCY_PROMPT_TEMPLATE, 
    SIMPLE_RESPONSE_PROMPT_TEMPLATE,
    INTENT_CLASSIFICATION_PROMPT_TEMPLATE,
    ANALYZE_MISSING_INFO_PROMPT_TEMPLATE,
    CREATE_QUERY_FROM_INFO_PROMPT_TEMPLATE,
    ASK_FOR_INFO_PROMPT_TEMPLATE # [ì¶”ê°€]
)
from app.agent.tools import milvus_knowledge_search, report_emergency, retrieve_qna
from app.services.qna_service import format_qna_docs
from app.dto.qna import QnADoc
from app.dto.rag import RagDoc
from app.core.llm_factory import get_generator_llm, get_evaluator_llm
from app.agent.utils import parse_tool_result, parse_json_from_response
import logging

logger = logging.getLogger(__name__)


async def intent_classifier_node(state: AgentState) -> AgentState:
    """
    ì˜ë„ ë¶„ë¥˜ ë…¸ë“œ
    ì§ˆë¬¸ì´ 'ë¯¸ìˆ™ì•„ ëŒë´„' ë²”ìœ„ì¸ì§€ íŒë‹¨ + 'ë¶€ì¡±í•œ ì •ë³´ ì œê³µ' ì—¬ë¶€ íŒë‹¨
    """
    logger.info("--- ğŸ¤– ì˜ë„ ë¶„ë¥˜ ë…¸ë“œ ì‹¤í–‰ ---")
    
    # missing_info ìˆìœ¼ë©´ provide_missing_infoë¡œ ì„¤ì •
    missing_info_data = state.get("_missing_info")
    missing_info = missing_info_data.get("missing_info", []) if missing_info_data else []
        
    if missing_info:
        logger.info(f"âœ… ë¶€ì¡±í•œ ì •ë³´ ìš”ì²­ ìƒíƒœ(missing_info ì¡´ì¬) -> ê°•ì œë¡œ provide_missing_infoë¡œ ì„¤ì •")
        state["_intent"] = "provide_missing_info"
        return state
    
    question = state.get("question", "")
    
    llm = get_evaluator_llm()
    if not llm:
        logger.warning("í‰ê°€ ëª¨ë¸ ì—†ìŒ, ê¸°ë³¸ê°’(relevant) ì„¤ì •")
        state["_intent"] = "irrelevant"
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        return state
        
    try:
        prompt = INTENT_CLASSIFICATION_PROMPT_TEMPLATE.format(
            question=question
        )
        messages = [HumanMessage(content=prompt)]
        
        response = await llm.ainvoke(messages)
        response_text = response.content.strip()
        
        result = parse_json_from_response(response_text)
        
        intent = result.get("intent", "relevant")
        reason = result.get("reason", "")
        
        logger.info(f"ì˜ë„ ë¶„ë¥˜ ê²°ê³¼: {intent} (ì´ìœ : {reason})")
        state["_intent"] = intent
        
        # irrelevantì¸ ê²½ìš° ì¦‰ì‹œ ë‹µë³€ ìƒì„±
        if intent == "irrelevant":
            logger.info("ğŸš« ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ -> ì¦‰ì‹œ ê±°ì ˆ ì‘ë‹µ ìƒì„±")
            try:
                simple_prompt = SIMPLE_RESPONSE_PROMPT_TEMPLATE.format(question=question)
                gen_llm = get_generator_llm()
                if gen_llm:

                    # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ íƒœê·¸ ì¶”ê°€
                    resp = await gen_llm.ainvoke(
                        [HumanMessage(content=simple_prompt)],
                        config={"tags": ["stream_response"]}
                    )
                    state["response"] = resp.content.strip()
                    state["messages"] = [resp]
                else:
                    state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¯¸ìˆ™ì•„ ë° ì‹ ìƒì•„ ëŒë´„ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            except Exception as ex:
                logger.error(f"ê±°ì ˆ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(ex)}")
                state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
    except Exception as e:
        logger.error(f"ì˜ë„ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        state["_intent"] = "relevant"
        
    return state


async def create_query_from_info_node(state: AgentState) -> AgentState:
    """
    Create Query From Info Node
    ë¶€ì¡±í–ˆë˜ ì •ë³´ê°€ ì œê³µë˜ë©´, ì´ë¥¼ ì›ë³¸ ì§ˆë¬¸ê³¼ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ê²€ìƒ‰ ì§ˆë¬¸ì„ ìƒì„±
    """
    logger.info("--- ğŸ¤– ì§ˆë¬¸ ì¬êµ¬ì„± ë…¸ë“œ ì‹¤í–‰í–‰ ---")
    
    # missing_info ë°ì´í„° êµ¬ì¡° ì²˜ë¦¬ (íƒ€ì… ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
    missing_info_data = state.get("_missing_info") or {}
    missing_info = missing_info_data.get("missing_info", []) if isinstance(missing_info_data, dict) else []
    saved_previous_question = missing_info_data.get("pending_question", "") if isinstance(missing_info_data, dict) else ""
        
    # ì €ì¥ëœ ì›ë³¸ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ í˜„ì¬ stateì˜ ì›ë³¸ ì§ˆë¬¸(í˜„ì¬ í„´ ì…ë ¥) ì‚¬ìš©
    previous_question = saved_previous_question if saved_previous_question else state.get("previous_question", "")
    
    logger.info(f"ì´ì „ ì§ˆë¬¸: {previous_question}")
    user_response = state.get("question", "") # í˜„ì¬ í„´ì˜ ì‚¬ìš©ì ì…ë ¥(ì •ë³´ ì œê³µ)
    
    llm = get_generator_llm()
    if not llm:
        return state
    
    missing_info_text = ", ".join(missing_info) if missing_info else ""
        
    prompt = CREATE_QUERY_FROM_INFO_PROMPT_TEMPLATE.format(
        previous_question=previous_question,
        missing_info=missing_info_text,
        user_response=user_response
    )
    
    try:
        # [Async] invoke -> ainvoke
        response = await llm.ainvoke([HumanMessage(content=prompt)])
        new_query = response.content.strip()
        
        logger.info(f"ìƒˆë¡œìš´ ê²€ìƒ‰ ì§ˆë¬¸ ìƒì„±: '{new_query}'")
        
        # ìƒì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ question ì—…ë°ì´íŠ¸
        state["question"] = new_query
        
        # missing_info ì´ˆê¸°í™” (í•´ê²°ë¨)
        state["_missing_info"] = None
        
        # ì¬ì‹œë„ í”Œë˜ê·¸ ì„¤ì • (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        state["is_retry"] = True
        
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì§ˆë¬¸ê³¼ ì‚¬ìš©ì ì…ë ¥ì„ ë‹¨ìˆœ ê²°í•©
        state["question"] = f"{previous_question} {user_response}"
        
    return state


async def agent_node(state: AgentState) -> AgentState:
    """
    í•µì‹¬ ì—ì´ì „íŠ¸ ë…¸ë“œ (Self-RAG)
    - ì§ˆë¬¸ ë¶„ì„ ë° tool í˜¸ì¶œ ê²°ì •
    - Tool í˜¸ì¶œì´ í•„ìš”í•˜ë©´ tool í˜¸ì¶œ
    - ì´ì „ ë‹¨ê³„ì˜ Tool ì‹¤í–‰ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ì—¬ State ì—…ë°ì´íŠ¸
    """
    logger.info("--- ğŸ¤– Agent ë…¸ë“œ ì‹¤í–‰ ---")

    # 1. ToolMessage ì²˜ë¦¬ ë° State ì—…ë°ì´íŠ¸
    messages = state.get("messages", [])
    new_retrieved_docs = []
    new_qna_docs = []
    
    # ë©”ì‹œì§€ë¥¼ ì—­ìˆœìœ¼ë¡œ í™•ì¸í•˜ë©° ê°€ì¥ ìµœê·¼ì˜ ToolMessageë“¤ì„ ë¶„ì„
    for msg in reversed(messages):
        # HumanMessageê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì˜ ToolMessageë“¤ë§Œ ìœ íš¨
        if isinstance(msg, HumanMessage):
            break
        if isinstance(msg, AIMessage):
            continue 
            
        if isinstance(msg, ToolMessage):
            tool_name = getattr(msg, "name", "")
            content = msg.content
            
            logger.info(f"ğŸ” ToolMessage ë¶„ì„: {tool_name}")
            
            if tool_name == "milvus_knowledge_search":
                docs = parse_tool_result(content)
                if docs:
                    for d in docs:
                        try:
                            # ë”•ì…”ë„ˆë¦¬ë¥¼ RagDoc ê°ì²´ë¡œ ë³€í™˜
                            rag_doc = RagDoc(**d)
                            new_retrieved_docs.append(rag_doc)
                        except Exception as e:
                            logger.error(f"RagDoc ë³€í™˜ ì‹¤íŒ¨: {e}")
                
            elif tool_name == "retrieve_qna":
                docs = parse_tool_result(content)
                if docs:
                    for d in docs:
                        try:
                            qna_doc = QnADoc(**d)
                            new_qna_docs.append(qna_doc)
                        except Exception as e:
                            logger.error(f"QnADoc ë³€í™˜ ì‹¤íŒ¨: {e}")

            elif tool_name == "report_emergency":
                logger.info("  -> ì‘ê¸‰ ìƒí™© ë³´ê³  í™•ì¸")
                state["is_emergency"] = True

    # State ì—…ë°ì´íŠ¸
    if new_retrieved_docs:
        state["_retrieved_docs"] = new_retrieved_docs
        logger.info(f"âœ… RAG ë¬¸ì„œ State ì—…ë°ì´íŠ¸: {len(new_retrieved_docs)}ê°œ")
        
    if new_qna_docs:
        state["_qna_docs"] = new_qna_docs
        logger.info(f"âœ… QnA ë¬¸ì„œ State ì—…ë°ì´íŠ¸: {len(new_qna_docs)}ê°œ")

    # 2. Agent ì‹¤í–‰ (LLM í˜¸ì¶œ)
    question = state.get("question", "")
    baby_info = state.get("baby_info", {})
    
    llm = get_generator_llm()
    if not llm:
        logger.error("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        return state
    
    try:
        # bind_tools ì‚¬ìš©í•˜ì—¬ íˆ´ ë°”ì¸ë”©
        tools = [
            milvus_knowledge_search,  # RAG ê²€ìƒ‰ tool
            report_emergency,         # ì‘ê¸‰ ìƒíƒœ ë³´ê³  tool
            retrieve_qna,             # QnA ê²€ìƒ‰ tool
        ]
        model_with_tools = llm.bind_tools(tools)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì•„ê¸° ì •ë³´ í¬í•¨)
        baby_context = get_baby_context_string(baby_info)
        
        # [ìˆ˜ì •] system_prompt ì¸ì ì œê±° (í…œí”Œë¦¿ì— í†µí•©ë¨)
        system_prompt = AGENT_NODE_PROMPT_TEMPLATE.format(
            baby_context=baby_context,
            question=question
        )
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
        messages_with_system = [SystemMessage(content=system_prompt)] + messages
        
        # Agent ì‹¤í–‰
        response = await model_with_tools.ainvoke(messages_with_system)

        # ì‘ë‹µì„ ë©”ì‹œì§€ì— ì¶”ê°€
        state["messages"] = [response]
        
    except Exception as e:
        logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        state["is_emergency"] = False
    
    return state


async def evaluate_node(state: AgentState) -> AgentState:
    """
    Grade Documents Node (Self-RAG)
    ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì§ˆë¬¸ ê´€ë ¨ì„±ì„ í‰ê°€
    """
    logger.info("--- ğŸ¤– í‰ê°€ ë…¸ë“œ ì‹¤í–‰ ---")
    question = state.get("question") or state.get("previous_question")
    
    retrieved_docs = state.get("_retrieved_docs", [])
    qna_docs = state.get("_qna_docs", []) or []

    if not retrieved_docs and not qna_docs:
        logger.warning(f"âš ï¸ í‰ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (RAG ë° QnA ëª¨ë‘ ì—†ìŒ).")
        state["_doc_relevance_score"] = 0.0
        state["_doc_relevance_passed"] = False
        return state
    
    llm = get_evaluator_llm()
    if not llm:
        logger.warning("âš ï¸ í‰ê°€ ëª¨ë¸ì´ ì—†ì–´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        state["_doc_relevance_score"] = 0.5
        state["_doc_relevance_passed"] = True
        return state
    
    try:
        # í‰ê°€ ëŒ€ìƒ êµ¬ì„±
        rag_to_evaluate = retrieved_docs[:5]
        qna_to_evaluate = qna_docs[:3]
        
        docs_summary = ""
        current_idx = 1
        
        # RAG ë¬¸ì„œ ìš”ì•½ ì¶”ê°€
        for doc in rag_to_evaluate:
            content = getattr(doc, "content", "")
            docs_summary += f"\në¬¸ì„œ {current_idx} (ì¼ë°˜ ë¬¸ì„œ):\n{content}\n"
            current_idx += 1
            
        # QnA ë¬¸ì„œ ìš”ì•½ ì¶”ê°€
        for doc in qna_to_evaluate:
            # Pydantic ëª¨ë¸ ì ‘ê·¼
            q = getattr(doc, "question", "")
            a = getattr(doc, "answer", "")
            docs_summary += f"\në¬¸ì„œ {current_idx} (QnA):\nQ: {q}\nA: {a}\n"
            current_idx += 1
        
        evaluation_prompt = DOC_RELEVANCE_PROMPT_TEMPLATE.format(
            question=question,
            docs_summary=docs_summary
        )
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ì •í™•í•˜ê²Œ í‰ê°€í•˜ì„¸ìš”."),
            HumanMessage(content=evaluation_prompt)
        ]
        
        response = await llm.ainvoke(messages)
        response_text = response.content.strip()
        
        evaluation_result = parse_json_from_response(response_text)
        
        score = float(evaluation_result.get("score", 0.5))
        
        # ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œ ì¸ë±ìŠ¤ ì¶”ì¶œ (1-based index)
        relevant_indices = evaluation_result.get("relevant_indices", [])

        state["_doc_relevance_score"] = max(0.0, min(1.0, score))
        state["_doc_relevance_passed"] = score >= 0.7
        logger.info(f"âœ… ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼: {evaluation_result.get('reason', '')}")
        logger.info(f"ì ìˆ˜={score:.2f}, í†µê³¼={state['_doc_relevance_passed']}")
        
        # [ìˆ˜ì •] RAGì™€ QnA ë¶„ë¦¬í•˜ì—¬ í•„í„°ë§
        filtered_rag = []
        filtered_qna = []
        
        rag_count = len(rag_to_evaluate)
        
        if relevant_indices:
            for idx in relevant_indices:
                # 1-based index -> 0-based
                real_idx = idx - 1 
                
                if real_idx < rag_count:
                    # RAG ë¬¸ì„œ ë²”ìœ„
                    filtered_rag.append(rag_to_evaluate[real_idx])
                else:
                    # QnA ë¬¸ì„œ ë²”ìœ„
                    qna_idx = real_idx - rag_count
                    if qna_idx < len(qna_to_evaluate):
                        filtered_qna.append(qna_to_evaluate[qna_idx])
        
        # í•„í„°ë§ ê²°ê³¼ ì ìš©
        logger.info(f"ê´€ë ¨ì„± í•„í„°ë§ (RAG): {len(retrieved_docs)} -> {len(filtered_rag)}")
        logger.info(f"ê´€ë ¨ì„± í•„í„°ë§ (QnA): {len(qna_docs)} -> {len(filtered_qna)}")
        
        state["_retrieved_docs"] = filtered_rag
        state["_qna_docs"] = filtered_qna # í•„í„°ë§ëœ QnAë¡œ êµì²´
    
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ í‰ê°€ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["_doc_relevance_score"] = 0.5
        state["_doc_relevance_passed"] = True
    
    return state


async def analyze_missing_info_node(state: AgentState) -> AgentState:
    """
    Analyze Missing Info Node
    ë¬¸ì„œê°€ ë¶ˆì¶©ë¶„í•  ë•Œ ì‚¬ìš©ìì—ê²Œ í•„ìš”í•œ ì •ë³´ì™€ ê·¸ ì´ìœ ë¥¼ ë¶„ì„
    """
    logger.info("--- ğŸ¤– ë¶€ì¡±í•œ ì •ë³´ ë¶„ì„ ë…¸ë“œ ì‹¤í–‰ ---")
    question = state.get("question") or state.get("previous_question", "")
    baby_info = state.get("baby_info", {})
    
    llm = get_evaluator_llm()
    if not llm:
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤."
        return state
        
    baby_context = get_baby_context_string(baby_info)
    
    prompt = ANALYZE_MISSING_INFO_PROMPT_TEMPLATE.format(
        question=question,
        baby_context=baby_context
    )
    
    try:
        response = await llm.ainvoke([HumanMessage(content=prompt)])
        response_text = response.content.strip()
        
        # JSON íŒŒì‹±
        result = parse_json_from_response(response_text)
        
        # ëˆ„ë½ ì •ë³´ ë° ì´ìœ  ì¶”ì¶œ
        missing_info_list = result.get("missing_info", [])
        reason = result.get("reason", "")
        
        logger.info(f"ë¶€ì¡±í•œ ì •ë³´ ë¶„ì„ ì™„ë£Œ")
        logger.info(f"ëˆ„ë½ ì •ë³´: {missing_info_list}, ì´ìœ : {reason}")
        
        # missing_info í•„ë“œì— ì €ì¥
        state["_missing_info"] = {
            "missing_info": missing_info_list,
            "reason": reason,
            "pending_question": question
        }
        
    except Exception as e:
        logger.error(f"ë¶€ì¡±í•œ ì •ë³´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        state["_missing_info"] = None
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
    return state


async def generate_node(state: AgentState) -> AgentState:
    """
    Generate Node (Self-RAG)
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± ë˜ëŠ” ë¶€ì¡±í•œ ì •ë³´ ìš”ì²­
    """
    logger.info("--- ğŸ¤– ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹¤í–‰ ---")
    question = state.get("question") or state.get("previous_question", "")
    baby_info = state.get("baby_info", {})
    
    missing_info_data = state.get("_missing_info")
    
    # 1. ì •ë³´ ë¶€ì¡± ì‹œ ì§ˆë¬¸ ìƒì„± ëª¨ë“œ
    if missing_info_data and isinstance(missing_info_data, dict):
        logger.info("ğŸ“ Missing Info Question Generation Mode")
        
        missing_info_list = missing_info_data.get("missing_info", [])
        reason = missing_info_data.get("reason", "")
        
        if not missing_info_list:
            # ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì—ˆë‹¤ë©´ ê·¸ëƒ¥ ì¼ë°˜ ë‹µë³€ ëª¨ë“œë¡œ ì§„í–‰ (í˜¹ì€ ì—ëŸ¬ ì²˜ë¦¬)
            logger.warning("missing_info ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ ì¼ë°˜ ë‹µë³€ ëª¨ë“œë¡œ ì „í™˜")
            missing_info_data = None
        else:
            baby_context = get_baby_context_string(baby_info)
            missing_info_str = ", ".join(missing_info_list)
            
            prompt = ASK_FOR_INFO_PROMPT_TEMPLATE.format(
                baby_context=baby_context,
                question=question,
                missing_info=missing_info_str,
                reason=reason
            )
            
            try:
                llm = get_generator_llm()
                if not llm:
                    state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    return state
                    
                response = await llm.ainvoke(
                    [SystemMessage(content=prompt)],
                    config={"tags": ["stream_response"]}
                )
                generated_response = response.content.strip()
                state["response"] = generated_response
                state["messages"] = [response]
                state["is_emergency"] = False
                state["_retrieved_docs"] = []
                state["_qna_docs"] = []
                
                return state
                
            except Exception as e:
                logger.error(f"ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
                state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                return state

    # 2. ì¼ë°˜ ë‹µë³€ ìƒì„± ëª¨ë“œ
    
    retrieved_docs = state.get("_retrieved_docs", [])
    qna_docs = state.get("_qna_docs", [])

    llm = get_generator_llm()
    if not llm:
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return state
    
    try:
        baby_context = get_baby_context_string(baby_info)
        
        # QnAì™€ RAG ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ê³µí†µ ë¡œì§)
        formatted_qna = format_qna_docs(qna_docs) if qna_docs else ""
        rag_context = get_docs_context_string(retrieved_docs)
        
        docs_context = ""
        if formatted_qna:
            docs_context += f"[QnA ì •ë³´]\n{formatted_qna}\n\n"
        if rag_context:
            docs_context += f"[ê²€ìƒ‰ëœ ë¬¸ì„œ]\n{rag_context}\n\n"
        
        if not docs_context:
            docs_context = "ê´€ë ¨ëœ ì°¸ì¡° ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì „ë¬¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        # ì‘ê¸‰/ì¼ë°˜ ëª¨ë“œì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        prompt = ""
        
        if state.get("is_emergency"):
            logger.info("ğŸš¨ Emergency Mode: ì‘ê¸‰ í”„ë¡¬í”„íŠ¸ ì ìš©")
            prompt = EMERGENCY_PROMPT_TEMPLATE.format(
                baby_context=baby_context,
                full_context=docs_context,
                previous_question=question
            )
        else:
            prompt = RESPONSE_GENERATION_PROMPT_TEMPLATE.format(
                baby_context=baby_context,
                docs_context=docs_context
            )
        
        response = await llm.ainvoke(
            [
                SystemMessage(content=prompt),
                HumanMessage(content=question)
            ],
            config={"tags": ["stream_response"]}
        )

        generated_response = response.content.strip()
        state["response"] = generated_response
        state["is_emergency"] = False
        
        # [ì¶”ê°€] ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìœ¼ë¯€ë¡œ, ë¶€ì¡±í•œ ì •ë³´ ìš”ì²­ ìƒíƒœ ì´ˆê¸°í™”
        state["_missing_info"] = None 
        
        # ë©”ì‹œì§€ì— ì¶”ê°€
        state["messages"] = [response]
        
    except Exception as e:
        logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        state["is_emergency"] = False
        state["_missing_info"] = None
    
    return state
