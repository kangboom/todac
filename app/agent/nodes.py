"""
ë…¸ë“œ í•¨ìˆ˜ (Self-RAG êµ¬ì¡°)
"""
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AIMessage
from app.agent.state import AgentState
from app.agent.prompts import (
    DOC_RELEVANCE_PROMPT_TEMPLATE, 
    RESPONSE_GENERATION_PROMPT_TEMPLATE,
    AGENT_NODE_PROMPT_TEMPLATE,
    get_baby_context_string,
    get_docs_context_string,
    EMERGENCY_PROMPT_TEMPLATE, 
    SIMPLE_RESPONSE_PROMPT_TEMPLATE,
    INTENT_CLASSIFICATION_PROMPT_TEMPLATE,
    ANALYZE_MISSING_INFO_PROMPT_TEMPLATE,
    CREATE_QUERY_FROM_INFO_PROMPT_TEMPLATE # [ì¶”ê°€]
)
from app.agent.tools import milvus_knowledge_search, report_emergency, retrieve_qna
from app.services.qna_service import format_qna_docs
from app.dto.qna import QnADoc
from app.dto.rag import RagDoc
from app.core.config import settings
import logging
import json

logger = logging.getLogger(__name__)

# LangChain OpenAI í´ë¼ì´ì–¸íŠ¸ (ì—ì´ì „íŠ¸ìš©)
agent_chat_model = ChatOpenAI(
    api_key=settings.OPENAI_API_KEY,
    model=settings.OPENAI_MODEL_GENERATION,
    temperature=0.7,
    max_tokens=1000
) if settings.OPENAI_API_KEY else None

# LangChain OpenAI í´ë¼ì´ì–¸íŠ¸ (í‰ê°€ìš© - ë‚®ì€ temperature)
evaluation_chat_model = ChatOpenAI(
    api_key=settings.OPENAI_API_KEY,
    model=settings.OPENAI_MODEL_GENERATION,
    temperature=0.1,  # í‰ê°€ëŠ” ë‚®ì€ temperature ì‚¬ìš©
    max_tokens=600
) if settings.OPENAI_API_KEY else None


def _parse_tool_result(content: str | list) -> list:
    """ToolMessageì˜ contentë¥¼ íŒŒì‹±í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    if isinstance(content, list):
        return content
    if isinstance(content, str):
        try:
            # JSON ë¬¸ìì—´ íŒŒì‹±
            parsed = json.loads(content)
            if isinstance(parsed, list):
                return parsed
            return []
        except json.JSONDecodeError:
            return []
    return []


def _parse_json_from_response(text: str) -> dict:
    """LLM ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSONì„ ì¶”ì¶œí•˜ì—¬ íŒŒì‹±"""
    try:
        text = text.strip()
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0].strip()
        elif "```" in text:
            text = text.split("```")[1].split("```")[0].strip()
        
        return json.loads(text)
    except json.JSONDecodeError:
        logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {text[:50]}...")
        return {}
    except Exception as e:
        logger.error(f"JSON ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {}


async def intent_classifier_node(state: AgentState) -> AgentState:
    """
    ì˜ë„ ë¶„ë¥˜ ë…¸ë“œ
    ì§ˆë¬¸ì´ 'ë¯¸ìˆ™ì•„ ëŒë´„' ë²”ìœ„ì¸ì§€ íŒë‹¨ + 'ë¶€ì¡±í•œ ì •ë³´ ì œê³µ' ì—¬ë¶€ íŒë‹¨
    """
    logger.info("--- [NODE] Intent Classification Start ---")
    question = state.get("question", "") or state.get("previous_question", "")
    
    # missing_info ë°ì´í„° êµ¬ì¡° ì²˜ë¦¬ (Dict or List or None)
    missing_info_data = state.get("_missing_info")
    missing_info = []
    
    if isinstance(missing_info_data, dict):
        missing_info = missing_info_data.get("missing_info", [])
    elif isinstance(missing_info_data, list):
        missing_info = missing_info_data
    
    if not state.get("previous_question"):
        state["previous_question"] = question
        
    # [ì¶”ê°€] missing_infoê°€ ìˆë‹¤ë©´ ë¬´ì¡°ê±´ provide_missing_infoë¡œ ì„¤ì • (LLM íŒë‹¨ ìƒëµ)
    if missing_info:
        logger.info(f"âœ… ë¶€ì¡±í•œ ì •ë³´ ìš”ì²­ ìƒíƒœ(missing_info ì¡´ì¬) -> ê°•ì œë¡œ provide_missing_infoë¡œ ì„¤ì •")
        state["_intent"] = "provide_missing_info"
        return state

    if not evaluation_chat_model:
        logger.warning("í‰ê°€ ëª¨ë¸ ì—†ìŒ, ê¸°ë³¸ê°’(relevant) ì„¤ì •")
        state["_intent"] = "relevant"
        return state
        
    try:
        # missing_infoê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨, ì—†ìœ¼ë©´ "ì—†ìŒ"ìœ¼ë¡œ ì²˜ë¦¬
        missing_info_text = ", ".join(missing_info) if missing_info else "ì—†ìŒ"
        
        prompt = INTENT_CLASSIFICATION_PROMPT_TEMPLATE.format(
            question=question
        )
        messages = [HumanMessage(content=prompt)]
        
        # [Async] invoke -> ainvoke
        response = await evaluation_chat_model.ainvoke(messages)
        response_text = response.content.strip()
        
        # [ìˆ˜ì •] ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©
        result = _parse_json_from_response(response_text)
        
        intent = result.get("intent", "relevant")
        reason = result.get("reason", "")
        
        logger.info(f"ì˜ë„ ë¶„ë¥˜ ê²°ê³¼: {intent} (ì´ìœ : {reason})")
        state["_intent"] = intent
        
        # irrelevantì¸ ê²½ìš° ì¦‰ì‹œ ë‹µë³€ ìƒì„±
        if intent == "irrelevant":
            logger.info("ğŸš« ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ -> ì¦‰ì‹œ ê±°ì ˆ ì‘ë‹µ ìƒì„±")
            try:
                simple_prompt = SIMPLE_RESPONSE_PROMPT_TEMPLATE.format(question=question)
                # agent_chat_modelì„ ì‚¬ìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìƒì„±
                if agent_chat_model:
                    # [Async] invoke -> ainvoke
                    resp = await agent_chat_model.ainvoke([HumanMessage(content=simple_prompt)])
                    state["response"] = resp.content.strip()
                    state["messages"] = [resp]
                else:
                    state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¯¸ìˆ™ì•„ ë° ì‹ ìƒì•„ ëŒë´„ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            except Exception as ex:
                logger.error(f"ê±°ì ˆ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(ex)}")
                state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
    except Exception as e:
        logger.error(f"ì˜ë„ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        state["_intent"] = "relevant" # ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ relevantë¡œ ì²˜ë¦¬
        
    return state


async def create_query_from_info_node(state: AgentState) -> AgentState:
    """
    Create Query From Info Node
    ë¶€ì¡±í–ˆë˜ ì •ë³´ê°€ ì œê³µë˜ë©´, ì´ë¥¼ ì›ë³¸ ì§ˆë¬¸ê³¼ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ê²€ìƒ‰ ì§ˆë¬¸ì„ ìƒì„±
    """
    logger.info("--- [NODE] Create Query From Info Start ---")
    
    # missing_info ë°ì´í„° êµ¬ì¡° ì²˜ë¦¬
    missing_info_data = state.get("_missing_info") or {}
    missing_info = []
    saved_previous_question = ""
    
    if isinstance(missing_info_data, dict):
        missing_info = missing_info_data.get("missing_info", [])
        saved_previous_question = missing_info_data.get("pending_question", "")
    elif isinstance(missing_info_data, list):
        missing_info = missing_info_data
        
    # ì €ì¥ëœ ì›ë³¸ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ í˜„ì¬ stateì˜ ì›ë³¸ ì§ˆë¬¸(í˜„ì¬ í„´ ì…ë ¥) ì‚¬ìš©
    previous_question = saved_previous_question if saved_previous_question else state.get("previous_question", "")
    
    logger.info(f"â“ previous_question: {previous_question}")
    user_response = state.get("question", "") # í˜„ì¬ í„´ì˜ ì‚¬ìš©ì ì…ë ¥(ì •ë³´ ì œê³µ)
    
    if not agent_chat_model:
        return state
    
    missing_info_text = ", ".join(missing_info) if missing_info else ""
        
    prompt = CREATE_QUERY_FROM_INFO_PROMPT_TEMPLATE.format(
        previous_question=previous_question,
        missing_info=missing_info_text,
        user_response=user_response
    )
    
    try:
        # [Async] invoke -> ainvoke
        response = await agent_chat_model.ainvoke([HumanMessage(content=prompt)])
        new_query = response.content.strip()
        
        logger.info(f"ìƒˆë¡œìš´ ê²€ìƒ‰ ì§ˆë¬¸ ìƒì„±: '{new_query}'")
        
        # ìƒì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ question ì—…ë°ì´íŠ¸
        state["question"] = new_query
        
        # missing_info ì´ˆê¸°í™” (í•´ê²°ë¨)
        state["_missing_info"] = None
        
        # ì¬ì‹œë„ í”Œë˜ê·¸ ì„¤ì • (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        state["is_retry"] = True
        
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì§ˆë¬¸ê³¼ ì‚¬ìš©ì ì…ë ¥ì„ ë‹¨ìˆœ ê²°í•©
        state["question"] = f"{previous_question} {user_response}"
        
    return state


async def agent_node(state: AgentState) -> AgentState:
    """
    í•µì‹¬ ì—ì´ì „íŠ¸ ë…¸ë“œ (Self-RAG)
    - ì§ˆë¬¸ ë¶„ì„ ë° tool í˜¸ì¶œ ê²°ì •
    - Tool í˜¸ì¶œì´ í•„ìš”í•˜ë©´ tool í˜¸ì¶œ, ì—†ìœ¼ë©´ ì§ì ‘ ë‹µë³€
    - ì´ì „ ë‹¨ê³„ì˜ Tool ì‹¤í–‰ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ì—¬ State ì—…ë°ì´íŠ¸
    """
    logger.info("--- [NODE] Agent Analysis Start ---")

    # 1. ToolMessage ì²˜ë¦¬ ë° State ì—…ë°ì´íŠ¸
    messages = state.get("messages", [])
    new_retrieved_docs = []
    new_qna_docs = []
    
    # ë©”ì‹œì§€ë¥¼ ì—­ìˆœìœ¼ë¡œ í™•ì¸í•˜ë©° ê°€ì¥ ìµœê·¼ì˜ ToolMessageë“¤ì„ ë¶„ì„
    # (HumanMessageê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì˜ ToolMessageë“¤ë§Œ ìœ íš¨)
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            break
        if isinstance(msg, AIMessage):
            continue 
            
        if isinstance(msg, ToolMessage):
            tool_name = getattr(msg, "name", "")
            content = msg.content
            
            logger.info(f"ğŸ” ToolMessage ë¶„ì„: {tool_name}")
            
            if tool_name == "milvus_knowledge_search":
                docs = _parse_tool_result(content)
                if docs:
                    for d in docs:
                        try:
                            # ë”•ì…”ë„ˆë¦¬ë¥¼ RagDoc ê°ì²´ë¡œ ë³€í™˜
                            rag_doc = RagDoc(**d)
                            new_retrieved_docs.append(rag_doc)
                        except Exception as e:
                            logger.error(f"RagDoc ë³€í™˜ ì‹¤íŒ¨: {e}")
                    logger.info(f"  -> RAG ë¬¸ì„œ {len(docs)}ê°œ ë°œê²¬")
                
            elif tool_name == "retrieve_qna":
                docs = _parse_tool_result(content)
                if docs:
                    for d in docs:
                        try:
                            # ë”•ì…”ë„ˆë¦¬ë¥¼ QnADoc ê°ì²´ë¡œ ë³€í™˜ (í•„ë“œëª… ë§¤í•‘ ì£¼ì˜)
                            # Toolì—ì„œ ë°˜í™˜í•˜ëŠ” JSON í‚¤ì™€ QnADoc í•„ë“œê°€ ì¼ì¹˜í•´ì•¼ í•¨
                            qna_doc = QnADoc(**d)
                            new_qna_docs.append(qna_doc)
                        except Exception as e:
                            logger.error(f"QnADoc ë³€í™˜ ì‹¤íŒ¨: {e}")
                    logger.info(f"  -> QnA ë¬¸ì„œ {len(docs)}ê°œ ë°œê²¬")

            elif tool_name == "report_emergency":
                logger.info("  -> ì‘ê¸‰ ìƒí™© ë³´ê³  í™•ì¸")
                state["is_emergency"] = True

    # State ì—…ë°ì´íŠ¸ (ìƒˆë¡œìš´ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ë®ì–´ì“°ê¸°)
    if new_retrieved_docs:
        state["_retrieved_docs"] = new_retrieved_docs
        logger.info(f"âœ… RAG ë¬¸ì„œ State ì—…ë°ì´íŠ¸: {len(new_retrieved_docs)}ê°œ")
        
    if new_qna_docs:
        state["_qna_docs"] = new_qna_docs
        logger.info(f"âœ… QnA ë¬¸ì„œ State ì—…ë°ì´íŠ¸: {len(new_qna_docs)}ê°œ")

    # 2. Agent ì‹¤í–‰ (LLM í˜¸ì¶œ)
    question = state.get("question", "")
    baby_info = state.get("baby_info", {})
    
    if not agent_chat_model:
        logger.error("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        return state
    
    try:
        # bind_tools ì‚¬ìš©í•˜ì—¬ íˆ´ ë°”ì¸ë”©
        tools = [
            milvus_knowledge_search,  # RAG ê²€ìƒ‰ tool
            report_emergency,         # ì‘ê¸‰ ìƒíƒœ ë³´ê³  tool
            retrieve_qna,             # QnA ê²€ìƒ‰ tool
        ]
        model_with_tools = agent_chat_model.bind_tools(tools)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì•„ê¸° ì •ë³´ í¬í•¨)
        baby_context = get_baby_context_string(baby_info)
        
        # [ìˆ˜ì •] system_prompt ì¸ì ì œê±° (í…œí”Œë¦¿ì— í†µí•©ë¨)
        system_prompt = AGENT_NODE_PROMPT_TEMPLATE.format(
            baby_context=baby_context,
            question=question
        )
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
        messages_with_system = [SystemMessage(content=system_prompt)] + messages
        
        # Agent ì‹¤í–‰
        # [Async] invoke -> ainvoke
        response = await model_with_tools.ainvoke(messages_with_system)
        
        # íˆ´ í˜¸ì¶œ í™•ì¸í•˜ì—¬ is_emergency í”Œë˜ê·¸ ì„¤ì • (í˜„ì¬ í„´ì˜ í˜¸ì¶œ í™•ì¸)
        # ì´ë¯¸ ìœ„ì—ì„œ ì´ì „ í„´ì˜ report_emergencyëŠ” ì²˜ë¦¬í–ˆì§€ë§Œ, ì´ë²ˆ í„´ì— ë˜ ë¶€ë¥¼ ìˆ˜ë„ ìˆìŒ
        
        has_tool_calls = False
        if hasattr(response, 'tool_calls') and response.tool_calls:
            has_tool_calls = True
        elif isinstance(response, dict) and response.get('tool_calls'):
            has_tool_calls = True
            
        if has_tool_calls:
            tool_calls = getattr(response, 'tool_calls', []) or response.get('tool_calls', [])
            
            for tool_call in tool_calls:
                tool_name = tool_call.get('name')
                logger.info(f"ğŸ› ï¸ Tool Call ê°ì§€: {tool_name}")
                
                # ì‘ê¸‰ íˆ´ì´ í˜¸ì¶œë˜ë©´ í”Œë˜ê·¸ True ì„¤ì •
                if tool_name == 'report_emergency':
                    logger.info(f"ğŸš¨ ì‘ê¸‰ íˆ´ í˜¸ì¶œ ê°ì§€ -> ì‘ê¸‰ ëª¨ë“œ í™œì„±í™”")
                    state["is_emergency"] = True
            
            tool_calls_count = len(tool_calls)
            logger.info(f"Tool í˜¸ì¶œ ê²°ì •: {tool_calls_count}ê°œ tool í˜¸ì¶œ")
            
        else:
            # Tool í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì§ì ‘ ë‹µë³€ (AIMessage content ì‚¬ìš©)
            # í•˜ì§€ë§Œ ì—¬ê¸°ì„œ ë‹µë³€ì„ í™•ì •í•˜ì§€ ì•Šê³ , evaluate_nodeë¡œ ë„˜ê¸¸ ìˆ˜ë„ ìˆìŒ
            # ì¼ë‹¨ responseì— ë‹´ì•„ë‘ 
            state["response"] = str(response.content).strip()
            logger.info("ë„êµ¬ ì—†ì´ ì§ì ‘ ì‘ë‹µ ìƒì„±")

        # ì‘ë‹µì„ ë©”ì‹œì§€ì— ì¶”ê°€
        state["messages"] = [response]
        
    except Exception as e:
        logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        state["is_emergency"] = False
    
    return state


async def evaluate_node(state: AgentState) -> AgentState:
    """
    Grade Documents Node (Self-RAG)
    ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì§ˆë¬¸ ê´€ë ¨ì„±ì„ í‰ê°€
    """
    logger.info("--- [NODE] Grade Documents Start ---")
    question = state.get("previous_question") or state.get("question", "")
    
    # Stateì—ì„œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (Agent Nodeì—ì„œ ì´ë¯¸ ìˆ˜ì§‘ë¨)
    retrieved_docs = state.get("_retrieved_docs", [])
    qna_docs = state.get("_qna_docs", []) or []

    if not retrieved_docs and not qna_docs:
        logger.warning(f"í‰ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (RAG ë° QnA ëª¨ë‘ ì—†ìŒ).")
        state["_doc_relevance_score"] = 0.0
        state["_doc_relevance_passed"] = False
        return state
    
    if not evaluation_chat_model:
        logger.warning("í‰ê°€ ëª¨ë¸ì´ ì—†ì–´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        state["_doc_relevance_score"] = 0.5
        state["_doc_relevance_passed"] = True
        return state
    
    try:
        # í‰ê°€ ëŒ€ìƒ êµ¬ì„±
        # RAG ë¬¸ì„œëŠ” ìµœëŒ€ 5ê°œ, QnA ë¬¸ì„œëŠ” ìµœëŒ€ 3ê°œë¡œ ì œí•œ (í† í° ê³ ë ¤)
        rag_to_evaluate = retrieved_docs[:5]
        qna_to_evaluate = qna_docs[:3]
        
        docs_summary = ""
        current_idx = 1
        
        # RAG ë¬¸ì„œ ìš”ì•½ ì¶”ê°€
        for doc in rag_to_evaluate:
            content = getattr(doc, "content", "")
            docs_summary += f"\në¬¸ì„œ {current_idx} (ì¼ë°˜ ë¬¸ì„œ):\n{content}\n"
            current_idx += 1
            
        # QnA ë¬¸ì„œ ìš”ì•½ ì¶”ê°€
        for doc in qna_to_evaluate:
            # Pydantic ëª¨ë¸ ì ‘ê·¼
            q = getattr(doc, "question", "")
            a = getattr(doc, "answer", "")
            docs_summary += f"\në¬¸ì„œ {current_idx} (QnA):\nQ: {q}\nA: {a}\n"
            current_idx += 1
        
        evaluation_prompt = DOC_RELEVANCE_PROMPT_TEMPLATE.format(
            question=question,
            docs_summary=docs_summary
        )
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ì •í™•í•˜ê²Œ í‰ê°€í•˜ì„¸ìš”."),
            HumanMessage(content=evaluation_prompt)
        ]
        
        # [Async] invoke -> ainvoke
        response = await evaluation_chat_model.ainvoke(messages)
        response_text = response.content.strip()
        
        # [ìˆ˜ì •] ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©
        evaluation_result = _parse_json_from_response(response_text)
        
        logger.info(f"ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼: {evaluation_result}")
        score = float(evaluation_result.get("score", 0.5))
        reason = evaluation_result.get("reason", "")
        
        # ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œ ì¸ë±ìŠ¤ ì¶”ì¶œ (1-based index)
        relevant_indices = evaluation_result.get("relevant_indices", [])
        logger.info(f"ê´€ë ¨ ë¬¸ì„œ ì¸ë±ìŠ¤: {relevant_indices}")

        state["_doc_relevance_score"] = max(0.0, min(1.0, score))
        state["_doc_relevance_passed"] = score >= 0.6
        logger.info(f"ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€: ì ìˆ˜={score:.2f}, í†µê³¼={state['_doc_relevance_passed']}")
        
        # [ìˆ˜ì •] RAGì™€ QnA ë¶„ë¦¬í•˜ì—¬ í•„í„°ë§
        filtered_rag = []
        filtered_qna = []
        
        rag_count = len(rag_to_evaluate)
        
        if relevant_indices:
            for idx in relevant_indices:
                # 1-based index -> 0-based
                real_idx = idx - 1 
                
                if real_idx < rag_count:
                    # RAG ë¬¸ì„œ ë²”ìœ„
                    filtered_rag.append(rag_to_evaluate[real_idx])
                else:
                    # QnA ë¬¸ì„œ ë²”ìœ„
                    qna_idx = real_idx - rag_count
                    if qna_idx < len(qna_to_evaluate):
                        filtered_qna.append(qna_to_evaluate[qna_idx])
        
        # í•„í„°ë§ ê²°ê³¼ ì ìš©
        logger.info(f"ê´€ë ¨ì„± í•„í„°ë§ (RAG): {len(retrieved_docs)} -> {len(filtered_rag)}")
        logger.info(f"ê´€ë ¨ì„± í•„í„°ë§ (QnA): {len(qna_docs)} -> {len(filtered_qna)}")
        
        state["_retrieved_docs"] = filtered_rag
        state["_qna_docs"] = filtered_qna # í•„í„°ë§ëœ QnAë¡œ êµì²´
        
        # í•„í„°ë§ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì ìˆ˜ê°€ ë†’ì•„ë„ ì‹¤íŒ¨ ì²˜ë¦¬
        if not filtered_rag and not filtered_qna:
            logger.warning("ê´€ë ¨ ë¬¸ì„œê°€ í•˜ë‚˜ë„ ì—†ì–´ í‰ê°€ë¥¼ ì‹¤íŒ¨ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            state["_doc_relevance_passed"] = False
            state["_doc_relevance_score"] = 0.0
        
        # (ì¶œì²˜ ì—…ë°ì´íŠ¸ ë¡œì§ ì œê±° - Serviceì—ì„œ ì²˜ë¦¬)
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ í‰ê°€ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["_doc_relevance_score"] = 0.5
        state["_doc_relevance_passed"] = True
    
    return state


async def analyze_missing_info_node(state: AgentState) -> AgentState:
    """
    Analyze Missing Info Node
    ë¬¸ì„œê°€ ë¶ˆì¶©ë¶„í•  ë•Œ ì‚¬ìš©ìì—ê²Œ í•„ìš”í•œ ì •ë³´ë¥¼ ë˜ë¬»ëŠ” ì‘ë‹µ ìƒì„±
    """
    logger.info("--- [NODE] Analyze Missing Info Start ---")
    question = state.get("previous_question") or state.get("question", "")
    baby_info = state.get("baby_info", {})
    
    if not agent_chat_model:
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤."
        return state
        
    baby_context = get_baby_context_string(baby_info)
    
    prompt = ANALYZE_MISSING_INFO_PROMPT_TEMPLATE.format(
        question=question,
        baby_context=baby_context
    )
    
    try:
        # [Async] invoke -> ainvoke
        response = await agent_chat_model.ainvoke([HumanMessage(content=prompt)])
        response_text = response.content.strip()
        
        # JSON íŒŒì‹±
        result = _parse_json_from_response(response_text)
        
        # 1. ì‚¬ìš©ì ì‘ë‹µ ë©”ì‹œì§€ ì¶”ì¶œ
        generated_response = result.get("response", "ì£„ì†¡í•©ë‹ˆë‹¤. ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # 2. ëˆ„ë½ ì •ë³´ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        missing_info_list = result.get("missing_info", [])
        
        logger.info(f"ë¶€ì¡±í•œ ì •ë³´ ìš”ì²­ ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        logger.info(f"ëˆ„ë½ ì •ë³´ ëª©ë¡: {missing_info_list}")
        
        # ì‘ë‹µ ì„¤ì • (ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ)
        state["response"] = generated_response
        state["messages"] = [AIMessage(content=generated_response)]
        
        # missing_info í•„ë“œì— ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥ (ì›ë˜ ì§ˆë¬¸ ë³´ì¡´)
        state["_missing_info"] = {
            "missing_info": missing_info_list,
            "pending_question": question
        }
        
    except Exception as e:
        logger.error(f"ë¶€ì¡±í•œ ì •ë³´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  ë‚´ìš©ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        state["_missing_info"] = None # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì´ˆê¸°í™”
        
    return state


async def generate_node(state: AgentState) -> AgentState:
    """
    Generate Node (Self-RAG)
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± (Strategy B ì œê±° -> í†µí•© ë¡œì§)
    """
    logger.info("--- [NODE] Generate Answer Start ---")
    previous_question = state.get("previous_question") or state.get("question", "")
    baby_info = state.get("baby_info", {})
    
    # evaluate_nodeì—ì„œ í•„í„°ë§ëœ ë¬¸ì„œë“¤ ê°€ì ¸ì˜¤ê¸°
    retrieved_docs = state.get("_retrieved_docs", [])
    qna_docs = state.get("_qna_docs", [])

    if not agent_chat_model:
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return state
    
    try:
        baby_context = get_baby_context_string(baby_info)
        
        # --- Prompt Selection Logic ---
        prompt = ""
        mode_log = "Normal"
        
        # [ìˆ˜ì •] ì‘ê¸‰ ìƒí™© ì²˜ë¦¬ (ìµœìš°ì„ )
        if state.get("is_emergency"):
            mode_log = "Emergency"
            logger.info("ğŸš¨ Emergency Mode: ì‘ê¸‰ í”„ë¡¬í”„íŠ¸ ì ìš©")
            
            docs_context = get_docs_context_string(retrieved_docs)
            formatted_qna = format_qna_docs(qna_docs) if qna_docs else ""
            
            # ì»¨í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            full_context = ""
            if formatted_qna:
                full_context += f"[QnA ì •ë³´]\n{formatted_qna}\n\n"
            if docs_context:
                full_context += f"[ê²€ìƒ‰ëœ ë¬¸ì„œ]\n{docs_context}"
                
            prompt = EMERGENCY_PROMPT_TEMPLATE.format(
                baby_context=baby_context,
                full_context=full_context,
                previous_question=previous_question
            )
        else:
            # í†µí•©ëœ ì¼ë°˜ ìƒì„± ë¡œì§ (Green/Yellow/Red êµ¬ë¶„ ì—†ìŒ)
            logger.info("ğŸ“ Standard Generation Mode")
            
            docs_context = ""
            
            # QnA ë‚´ìš© ì¶”ê°€
            if qna_docs:
                formatted_qna = format_qna_docs(qna_docs)
                docs_context += f"[QnA ì •ë³´]\n{formatted_qna}\n\n"
                
            # RAG ë¬¸ì„œ ë‚´ìš© ì¶”ê°€
            if retrieved_docs:
                rag_context = get_docs_context_string(retrieved_docs)
                docs_context += f"{rag_context}"
                
            if not docs_context:
                docs_context = "ê´€ë ¨ëœ ì°¸ì¡° ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì „ë¬¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
            
            prompt = RESPONSE_GENERATION_PROMPT_TEMPLATE.format(
                baby_context=baby_context,
                docs_context=docs_context
            )
        
        # [ë¡œê¹…] ìµœì¢… ì‚¬ìš©ëœ ì¶œì²˜ ì •ë³´ ì¶œë ¥
        log_sources = []
        
        # QnA ì†ŒìŠ¤ ë¡œê¹…
        for doc in qna_docs:
            filename = getattr(doc, 'source', 'unknown')
            q_text = getattr(doc, 'question', '')
            if len(q_text) > 15:
                q_text = q_text[:15] + "..."
            log_sources.append(f"QnA '{q_text}': {filename}")
            
        # Doc ì†ŒìŠ¤ ë¡œê¹…
        for doc in retrieved_docs:
             filename = getattr(doc, 'filename', 'unknown')
             log_sources.append(f"Doc:{filename}")

        if log_sources:
            logger.info(f"ğŸ“š ìµœì¢… ì‚¬ìš©ëœ ì¶œì²˜ ({len(log_sources)}ê°œ): {', '.join(log_sources)}")
        else:
            logger.info("ğŸ“š ì‚¬ìš©ëœ ì¶œì²˜ ì—†ìŒ")

        # ë‹µë³€ ìƒì„±
        # [Async] invoke -> ainvoke
        response = await agent_chat_model.ainvoke([
            SystemMessage(content=prompt),
            HumanMessage(content=previous_question)
        ])
        
        generated_response = response.content.strip()
        state["response"] = generated_response
        state["is_emergency"] = False
        
        # [ì¶”ê°€] ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìœ¼ë¯€ë¡œ, ë¶€ì¡±í•œ ì •ë³´ ìš”ì²­ ìƒíƒœ ì´ˆê¸°í™”
        state["_missing_info"] = None 
        
        # ë©”ì‹œì§€ì— ì¶”ê°€
        state["messages"] = [response]
        
        logger.info(f"ë‹µë³€ ìƒì„± ì™„ë£Œ (ëª¨ë“œ: {mode_log})")
        
    except Exception as e:
        logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        state["is_emergency"] = False
        # ì‹¤íŒ¨ ì‹œì—ë„ ìƒíƒœë¥¼ ì´ˆê¸°í™”í• ì§€ ì—¬ë¶€ëŠ” ì„ íƒì‚¬í•­ì´ë‚˜, ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”
        state["_missing_info"] = None
    
    return state
