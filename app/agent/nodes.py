"""
ë…¸ë“œ í•¨ìˆ˜ (Self-RAG êµ¬ì¡°)
"""
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AIMessage
from app.agent.state import AgentState
from app.agent.prompts import (
    SYSTEM_PROMPT,  # Agent Nodeìš©
    DOC_RELEVANCE_PROMPT_TEMPLATE, 
    REWRITE_QUERY_PROMPT_TEMPLATE,
    HALLUCINATION_CHECK_PROMPT_TEMPLATE,
    RESPONSE_GENERATION_PROMPT_TEMPLATE,
    AGENT_NODE_PROMPT_TEMPLATE,
    get_baby_context_string,
    get_docs_context_string,
    PERSONA_PROMPT, # ê³µí†µ í˜ë¥´ì†Œë‚˜
    QNA_GREEN_PROMPT_TEMPLATE,
    QNA_YELLOW_PROMPT_TEMPLATE,
    EMERGENCY_PROMPT_TEMPLATE # [ë³€ê²½] ì‘ê¸‰ ìƒí™© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
)
from app.agent.tools import milvus_knowledge_search, report_emergency
from app.services.qna_service import search_qna, format_qna_docs
from app.core.config import settings
import logging
import json

logger = logging.getLogger(__name__)

# LangChain OpenAI í´ë¼ì´ì–¸íŠ¸ (ì—ì´ì „íŠ¸ìš©)
agent_chat_model = ChatOpenAI(
    api_key=settings.OPENAI_API_KEY,
    model=settings.OPENAI_MODEL_GENERATION,
    temperature=0.7,
    max_tokens=1000
) if settings.OPENAI_API_KEY else None

# LangChain OpenAI í´ë¼ì´ì–¸íŠ¸ (í‰ê°€ìš© - ë‚®ì€ temperature)
evaluation_chat_model = ChatOpenAI(
    api_key=settings.OPENAI_API_KEY,
    model=settings.OPENAI_MODEL_GENERATION,
    temperature=0.1,  # í‰ê°€ëŠ” ë‚®ì€ temperature ì‚¬ìš©
    max_tokens=200
) if settings.OPENAI_API_KEY else None


def retrieve_qna_node(state: AgentState) -> AgentState:
    """
    ê³µì‹ QnA ê²€ìƒ‰ ë…¸ë“œ
    ê°€ì¥ ë¨¼ì € ì‹¤í–‰ë˜ì–´ QnA DBë¥¼ ê²€ìƒ‰í•˜ê³  ì ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    logger.info("--- [NODE] QnA Retrieval Start ---")
    question = state.get("original_question") or state.get("question", "")
    
    # ì›ë³¸ ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ì €ì¥
    if not state.get("original_question"):
        state["original_question"] = question

    # QnA ê²€ìƒ‰ ì‹¤í–‰ (ë™ê¸° í˜¸ì¶œ)
    qna_results = search_qna(question)
    
    # ìµœê³  ì ìˆ˜ ê³„ì‚°
    max_score = 0.0
    if qna_results:
        # DTO ê°ì²´ì´ë¯€ë¡œ .score ì†ì„± ì ‘ê·¼
        max_score = max([doc.score for doc in qna_results])
        
    logger.info(f"QnA Search Result: Score={max_score:.2f}, Count={len(qna_results)}")
    
    # State ì—…ë°ì´íŠ¸
    state["qna_docs"] = qna_results
    state["qna_score"] = max_score
    
    return state


def agent_node(state: AgentState) -> AgentState:
    """
    í•µì‹¬ ì—ì´ì „íŠ¸ ë…¸ë“œ (Self-RAG)
    - ì§ˆë¬¸ ë¶„ì„ ë° tool í˜¸ì¶œ ê²°ì •
    - Tool í˜¸ì¶œì´ í•„ìš”í•˜ë©´ tool í˜¸ì¶œ, ì—†ìœ¼ë©´ ì§ì ‘ ë‹µë³€
    """
    logger.info("--- [NODE] Agent Analysis Start ---")

    question = state.get("question", "")
    messages = state.get("messages", [])
    baby_info = state.get("baby_info", {})
    
    if not agent_chat_model:
        logger.error("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        return state
    
    try:
        # [ìˆ˜ì •] bind_tools ì‚¬ìš©í•˜ì—¬ íˆ´ ë°”ì¸ë”© (í‘œì¤€ Tool Calling ë°©ì‹)
        tools = [
            milvus_knowledge_search,  # RAG ê²€ìƒ‰ tool
            report_emergency,         # ì‘ê¸‰ ìƒíƒœ ë³´ê³  tool
        ]
        model_with_tools = agent_chat_model.bind_tools(tools)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì•„ê¸° ì •ë³´ í¬í•¨)
        baby_context = get_baby_context_string(baby_info)
        
        system_prompt = AGENT_NODE_PROMPT_TEMPLATE.format(
            system_prompt=SYSTEM_PROMPT,
            baby_context=baby_context
        )
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
        messages_with_system = [SystemMessage(content=system_prompt)] + messages
        
        # Agent ì‹¤í–‰
        response = model_with_tools.invoke(messages_with_system)
        
        # [ë¡œì§] íˆ´ í˜¸ì¶œ í™•ì¸í•˜ì—¬ is_emergency í”Œë˜ê·¸ ì„¤ì •
        state["is_emergency"] = False # ì´ˆê¸°í™”
        
        # responseê°€ tool_calls ì†ì„±ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸ (Pydantic v1/v2 í˜¸í™˜ì„±)
        has_tool_calls = False
        if hasattr(response, 'tool_calls') and response.tool_calls:
            has_tool_calls = True
        elif isinstance(response, dict) and response.get('tool_calls'):
            has_tool_calls = True
            
        if has_tool_calls:
            tool_calls = getattr(response, 'tool_calls', []) or response.get('tool_calls', [])
            
            for tool_call in tool_calls:
                tool_name = tool_call.get('name')
                logger.info(f"ğŸ› ï¸ Tool Call ê°ì§€: {tool_name} (Args: {tool_call.get('args')})")
                
                # ì‘ê¸‰ íˆ´ì´ í˜¸ì¶œë˜ë©´ í”Œë˜ê·¸ True ì„¤ì •
                if tool_name == 'report_emergency':
                    logger.info(f"ğŸš¨ ì‘ê¸‰ íˆ´ í˜¸ì¶œ ê°ì§€ -> ì‘ê¸‰ ëª¨ë“œ í™œì„±í™”")
                    state["is_emergency"] = True
            
            tool_calls_count = len(tool_calls)
            logger.info(f"Tool í˜¸ì¶œ ê²°ì •: {tool_calls_count}ê°œ tool í˜¸ì¶œ")
            
        else:
            # Tool í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì§ì ‘ ë‹µë³€ (AIMessage content ì‚¬ìš©)
            state["response"] = str(response.content).strip()
            logger.info("ë„êµ¬ ì—†ì´ ì§ì ‘ ì‘ë‹µ ìƒì„±")

        # ì‘ë‹µì„ ë©”ì‹œì§€ì— ì¶”ê°€
        state["messages"] = [response]
        
        # Tool í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì§ì ‘ ë‹µë³€ (ì¤‘ë³µ ë¡œì§ ì œê±° ë° ì •ë¦¬)
        # should_continue ë…¸ë“œì—ì„œ tool_calls ìœ ë¬´ë¡œ íŒë‹¨í•¨
        
    except Exception as e:
        logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        state["is_emergency"] = False
    
    return state


def grade_documents_node(state: AgentState) -> AgentState:
    """
    Grade Documents Node (Self-RAG)
    ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì§ˆë¬¸ ê´€ë ¨ì„±ì„ í‰ê°€
    """
    logger.info("--- [NODE] Grade Documents Start ---")
    question = state.get("original_question") or state.get("question", "")
    messages = state.get("messages", [])
    
    # ë¨¼ì € stateì—ì„œ í™•ì¸
    retrieved_docs = state.get("retrieved_docs", [])
    
    # Stateì— ì—†ìœ¼ë©´ ToolMessageì—ì„œ ì¶”ì¶œ
    if not retrieved_docs:
        logger.info(f"ToolMessage ì¶”ì¶œ ì‹œì‘: messages ê°œìˆ˜={len(messages)}")
        for idx, msg in enumerate(reversed(messages)):
            if isinstance(msg, ToolMessage):
                tool_result = msg.content
                if isinstance(tool_result, list) and tool_result:
                    retrieved_docs = tool_result
                    logger.info(f"ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ ì„±ê³µ: {len(retrieved_docs)}ê°œ ë¬¸ì„œ")
                    state["retrieved_docs"] = retrieved_docs
                    
                    # RAG ì†ŒìŠ¤ ì •ë³´ ì €ì¥
                    rag_sources = [
                        {
                            "doc_id": str(doc.get("doc_id", "")),
                            "chunk_index": doc.get("chunk_index", ""),
                            "score": doc.get("score", 0.0),
                            "filename": doc.get("filename", ""),
                            "category": doc.get("category", "")
                        }
                        for doc in retrieved_docs
                    ]
                    state["rag_sources"] = rag_sources
                    break
                # JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”ëœ ê²½ìš°
                elif isinstance(tool_result, str):
                     try:
                        import json
                        parsed_result = json.loads(tool_result)
                        if isinstance(parsed_result, list) and parsed_result:
                            retrieved_docs = parsed_result
                            logger.info(f"JSON íŒŒì‹± ì„±ê³µ: {len(retrieved_docs)}ê°œ ë¬¸ì„œ")
                            state["retrieved_docs"] = retrieved_docs
                            
                            rag_sources = [
                                {
                                    "doc_id": str(doc.get("doc_id", "")),
                                    "chunk_index": doc.get("chunk_index", ""),
                                    "score": doc.get("score", 0.0),
                                    "filename": doc.get("filename", ""),
                                    "category": doc.get("category", "")
                                }
                                for doc in retrieved_docs
                            ]
                            state["rag_sources"] = rag_sources
                            break
                     except (json.JSONDecodeError, TypeError):
                        pass

    if not retrieved_docs:
        logger.warning(f"í‰ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        state["_doc_relevance_score"] = 0.0
        state["_doc_relevance_passed"] = False
        return state
    
    if not evaluation_chat_model:
        logger.warning("í‰ê°€ ëª¨ë¸ì´ ì—†ì–´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        state["_doc_relevance_score"] = 0.5
        state["_doc_relevance_passed"] = True
        return state
    
    try:
        # ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ í‰ê°€
        docs_to_evaluate = retrieved_docs[:3]
        
        docs_summary = ""
        for i, doc in enumerate(docs_to_evaluate, 1):
            content = doc.get('content', '')[:300]
            docs_summary += f"\në¬¸ì„œ {i}:\n{content}...\n"
        
        evaluation_prompt = DOC_RELEVANCE_PROMPT_TEMPLATE.format(
            question=question,
            docs_summary=docs_summary
        )
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ì •í™•í•˜ê²Œ í‰ê°€í•˜ì„¸ìš”."),
            HumanMessage(content=evaluation_prompt)
        ]
        
        response = evaluation_chat_model.invoke(messages)
        response_text = response.content.strip()
        
        # JSON íŒŒì‹± ë° ì ìˆ˜ ì¶”ì¶œ
        try:
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            evaluation_result = json.loads(response_text)
            logger.info(f"ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼: {evaluation_result}")
            score = float(evaluation_result.get("score", 0.5))
            reason = evaluation_result.get("reason", "")
            
            # [ì¶”ê°€] ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œ ì¸ë±ìŠ¤ ì¶”ì¶œ (1-based index)
            relevant_indices = evaluation_result.get("relevant_indices", [])
            
            logger.info(f"ê´€ë ¨ ë¬¸ì„œ ì¸ë±ìŠ¤: {relevant_indices}")

            state["_doc_relevance_score"] = max(0.0, min(1.0, score))
            state["_doc_relevance_passed"] = score >= 0.6
            logger.info(f"ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€: ì ìˆ˜={score:.2f}, í†µê³¼={state['_doc_relevance_passed']}")
            
            # [ìˆ˜ì •] ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë§Œ í•„í„°ë§í•˜ì—¬ ì €ì¥ (Pass ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ í•­ìƒ ì ìš©)
            # relevant_indicesê°€ ë¹„ì–´ìˆìœ¼ë©´ retrieved_docsë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ë¨ -> í™”ë©´ì— ì—‰ëš±í•œ ë¬¸ì„œ í‘œì‹œ ë°©ì§€
            filtered_docs = []
            if relevant_indices:
                # indicesëŠ” 1ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ -1 í•´ì¤Œ
                for idx in relevant_indices:
                    if 1 <= idx <= len(docs_to_evaluate):
                        filtered_docs.append(docs_to_evaluate[idx-1])
            
            # í•„í„°ë§ ê²°ê³¼ ì ìš©
            logger.info(f"ê´€ë ¨ì„± í•„í„°ë§: {len(retrieved_docs)}ê°œ -> {len(filtered_docs)}ê°œ")
            state["retrieved_docs"] = filtered_docs
            
            # [ì¶”ê°€] í•„í„°ë§ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì ìˆ˜ê°€ ë†’ì•„ë„ ì‹¤íŒ¨ ì²˜ë¦¬
            if not filtered_docs:
                logger.warning("ê´€ë ¨ ë¬¸ì„œê°€ ì—†ì–´ í‰ê°€ë¥¼ ì‹¤íŒ¨ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                state["_doc_relevance_passed"] = False
                state["_doc_relevance_score"] = 0.0
            
            # RAG ì†ŒìŠ¤ ì •ë³´ ì¬êµ¬ì„±
            if filtered_docs:
                rag_sources = [
                    {
                        "doc_id": str(doc.get("doc_id", "")),
                        "chunk_index": doc.get("chunk_index", ""),
                        "score": doc.get("score", 0.0),
                        "filename": doc.get("filename", ""),
                        "category": doc.get("category", "")
                    }
                    for doc in filtered_docs
                ]
                state["rag_sources"] = rag_sources
            else:
                state["rag_sources"] = []

        except Exception as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            state["_doc_relevance_score"] = 0.5
            state["_doc_relevance_passed"] = True
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ í‰ê°€ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["_doc_relevance_score"] = 0.5
        state["_doc_relevance_passed"] = True
    
    return state


def rewrite_query_node(state: AgentState) -> AgentState:
    """Rewrite Query Node"""
    logger.info("--- [NODE] Rewrite Query Start ---")
    original_question = state.get("original_question") or state.get("question", "")
    retrieved_docs = state.get("retrieved_docs", [])
    
    # [ì¶”ê°€] RAG ê²€ìƒ‰ ì‹œë„ íšŸìˆ˜ ì¦ê°€
    attempts = state.get("rag_retrieval_attempts", 0) + 1
    state["rag_retrieval_attempts"] = attempts
    logger.info(f"RAG ê²€ìƒ‰ ì¬ì‹œë„ íšŸìˆ˜: {attempts}")

    state["retrieved_docs"] = [] # ì¬ê²€ìƒ‰ì„ ìœ„í•´ ì´ˆê¸°í™”
    
    if not agent_chat_model:
        return state
    
    try:
        docs_summary = ""
        if retrieved_docs:
            docs_summary = "\nì´ì „ ê²€ìƒ‰ ê²°ê³¼ (ê´€ë ¨ì„±ì´ ë‚®ì•˜ìŒ):\n"
            for i, doc in enumerate(retrieved_docs[:2], 1):
                content = doc.get('content', '')[:150]
                docs_summary += f"{i}. {content}...\n"
        
        rewrite_prompt = REWRITE_QUERY_PROMPT_TEMPLATE.format(
            original_question=original_question,
            docs_summary=docs_summary
        )
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            HumanMessage(content=rewrite_prompt)
        ]
        
        response = agent_chat_model.invoke(messages)
        rewritten_query = response.content.strip()
        
        if not state.get("original_question"):
            state["original_question"] = original_question
        
        state["question"] = rewritten_query
        logger.info(f"ì§ˆë¬¸ ì¬êµ¬ì„±: '{original_question}' â†’ '{rewritten_query}'")
        
        # ì¬êµ¬ì„±ëœ ì§ˆë¬¸ì„ HumanMessageë¡œ ì¶”ê°€í•˜ì—¬ Agentê°€ ë‹¤ì‹œ ê²€ìƒ‰í•˜ë„ë¡ ìœ ë„
        new_message = HumanMessage(
            content=f"ì´ì „ ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì²´í™”í•˜ì—¬ ë‹¤ì‹œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”: '{rewritten_query}'"
        )
        # add_messages ë¦¬ë“€ì„œê°€ ë™ì‘í•˜ë„ë¡ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ë°˜í™˜
        state["messages"] = [new_message]
        
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ì¬êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
    
    return state


def generate_node(state: AgentState) -> AgentState:
    """
    Generate Node (Self-RAG + Strategy B)
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
    """
    logger.info("--- [NODE] Generate Answer Start ---")
    original_question = state.get("original_question") or state.get("question", "")
    baby_info = state.get("baby_info", {})
    retrieved_docs = state.get("retrieved_docs", [])
    messages = state.get("messages", [])
    
    # Strategy B State
    qna_score = state.get("qna_score", 0.0)
    qna_docs = state.get("qna_docs", [])
    
    attempts = state.get("_generation_attempts", 0) + 1
    state["_generation_attempts"] = attempts
    
    if not agent_chat_model:
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return state
    
    try:
        baby_context = get_baby_context_string(baby_info)
        
        # --- Prompt Selection Logic ---
        prompt = ""
        mode_log = "Red"
        
        # ë¡œê¹…ìš© ë³€ìˆ˜
        log_context = ""

        # [ìˆ˜ì •] ì‘ê¸‰ ìƒí™© ì²˜ë¦¬ (ìµœìš°ì„ )
        if state.get("is_emergency"):
            mode_log = "Emergency"
            logger.info("ğŸš¨ Emergency Mode: ì‘ê¸‰ í”„ë¡¬í”„íŠ¸ ì ìš©")
            
            docs_context = get_docs_context_string(retrieved_docs)
            formatted_qna = format_qna_docs(qna_docs) if qna_docs else ""
            
            # ì»¨í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            full_context = ""
            if formatted_qna:
                full_context += f"[QnA ì •ë³´]\n{formatted_qna}\n\n"
            if docs_context:
                full_context += f"[ê²€ìƒ‰ëœ ë¬¸ì„œ]\n{docs_context}"
                
            prompt = EMERGENCY_PROMPT_TEMPLATE.format(
                baby_context=baby_context,
                full_context=full_context,
                original_question=original_question
            )
        elif qna_score >= 0.9 and qna_docs:
            mode_log = "Green"
            logger.info("ğŸŸ¢ Green Mode: QnA Only Generation")
            
            formatted_qna = format_qna_docs(qna_docs)
            log_context = formatted_qna
            
            prompt = QNA_GREEN_PROMPT_TEMPLATE.format(
                question=original_question,
                qna_context=formatted_qna
            )
            
        elif qna_score >= 0.7 and qna_docs:
            mode_log = "Yellow"
            logger.info("ğŸŸ¡ Yellow Mode: Hybrid Generation")
            
            docs_context = get_docs_context_string(retrieved_docs)
            formatted_qna = format_qna_docs(qna_docs)
            log_context = f"QnA:\n{formatted_qna}\n\nDocs:\n{docs_context}"
            
            prompt = QNA_YELLOW_PROMPT_TEMPLATE.format(
                baby_context=baby_context,
                question=original_question,
                qna_context=formatted_qna,
                context=docs_context
            )
            
        else:
            # Red or Normal Mode
            logger.info("ğŸ”´ Red/Normal Mode: Standard RAG Generation")
            docs_context = get_docs_context_string(retrieved_docs)
            log_context = docs_context
            
            # RAG ì†ŒìŠ¤ ì •ë³´ ì €ì¥ (ì¼ë°˜ ê²€ìƒ‰ë§Œ í•´ë‹¹)
            if retrieved_docs:
                rag_sources = [
                    {
                        "doc_id": str(doc.get("doc_id", "")),
                        "chunk_index": doc.get("chunk_index", ""),
                        "score": doc.get("score", 0.0),
                        "filename": doc.get("filename", ""),
                        "category": doc.get("category", "")
                    }
                    for doc in retrieved_docs
                ]
                state["rag_sources"] = rag_sources
            
            prompt = RESPONSE_GENERATION_PROMPT_TEMPLATE.format(
                system_prompt=PERSONA_PROMPT,
                baby_context=baby_context,
                docs_context=docs_context
            )
        
        # ë‹µë³€ ìƒì„±
        response = agent_chat_model.invoke([
            SystemMessage(content=prompt),
            HumanMessage(content=original_question)
        ])
        
        generated_response = response.content.strip()
        state["response"] = generated_response
        state["is_emergency"] = False
        
        # ë©”ì‹œì§€ì— ì¶”ê°€
        state["messages"] = [response]
        
        logger.info(f"ë‹µë³€ ìƒì„± ì™„ë£Œ (ëª¨ë“œ: {mode_log}, ì‹œë„: {attempts})")
        
    except Exception as e:
        logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        state["is_emergency"] = False
    
    return state


def grade_hallucination_node(state: AgentState) -> AgentState:
    """
    Grade Hallucination Node
    """
    logger.info("--- [NODE] Grade Hallucination Start ---")
    question = state.get("original_question") or state.get("question", "")
    response = state.get("response", "")
    retrieved_docs = state.get("retrieved_docs", [])
    qna_docs = state.get("qna_docs", [])
    qna_score = state.get("qna_score", 0.0)
    
    if not response:
        state["_hallucination_score"] = 0.0
        state["_hallucination_passed"] = False
        return state
    
    if not evaluation_chat_model:
        state["_hallucination_score"] = 0.8
        state["_hallucination_passed"] = True
        return state
    
    try:
        # ê²€ì¦ ëŒ€ìƒ ë¬¸ì„œ ì„ íƒ
        context_docs = []
        mode_log = "Red"
        
        if qna_score >= 0.9 and qna_docs:
            mode_log = "Green"
            # QnADocëŠ” Pydantic ëª¨ë¸
            docs_summary = "\nì°¸ì¡° ë¬¸ì„œ (QnA):\n"
            for i, doc in enumerate(qna_docs[:3], 1):
                docs_summary += f"{i}. Q: {doc.question}\nA: {doc.answer}\n"
        elif qna_score >= 0.7:
            mode_log = "Yellow"
            docs_summary = "\nì°¸ì¡° ë¬¸ì„œ (QnA + General):\n"
            if qna_docs:
                for i, doc in enumerate(qna_docs[:2], 1):
                    docs_summary += f"QnA {i}: {doc.answer[:100]}...\n"
            if retrieved_docs:
                for i, doc in enumerate(retrieved_docs[:2], 1):
                    content = doc.get('content', '')
                    docs_summary += f"Doc {i}: {content[:100]}...\n"
        else:
            mode_log = "Red"
            docs_summary = "\nì°¸ì¡° ë¬¸ì„œ:\n"
            if retrieved_docs:
                for i, doc in enumerate(retrieved_docs[:3], 1):
                    content = doc.get('content', '')
                    docs_summary += f"{i}. {content[:200]}...\n"
            else:
                docs_summary = "\nì°¸ì¡° ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\n"
        
        evaluation_prompt = HALLUCINATION_CHECK_PROMPT_TEMPLATE.format(
            question=question,
            docs_summary=docs_summary,
            response=response
        )
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ë‹µë³€ì˜ ì •í™•ì„±ê³¼ í™˜ê°ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            HumanMessage(content=evaluation_prompt)
        ]
        
        eval_response = evaluation_chat_model.invoke(messages)
        response_text = eval_response.content.strip()
        
        try:
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            evaluation_result = json.loads(response_text)
            score = float(evaluation_result.get("score", 0.5))
            has_hallucination = evaluation_result.get("has_hallucination", False)
            
            state["_hallucination_score"] = max(0.0, min(1.0, score))
            state["_hallucination_passed"] = score >= 0.7 and not has_hallucination
            
            logger.info(f"í™˜ê° í‰ê°€ ({mode_log}): í†µê³¼={state['_hallucination_passed']}")
            
        except Exception:
            state["_hallucination_score"] = 0.7
            state["_hallucination_passed"] = True
        
    except Exception as e:
        logger.error(f"í™˜ê° í‰ê°€ ì‹¤íŒ¨: {str(e)}")
        state["_hallucination_score"] = 0.7
        state["_hallucination_passed"] = True
    
    return state
