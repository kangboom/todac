"""
ë…¸ë“œ í•¨ìˆ˜ (Self-RAG êµ¬ì¡°)
"""
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AIMessage
from app.agent.state import AgentState
from app.agent.prompts import (
    SIMPLE_RESPONSE_PROMPT_TEMPLATE,
    INTENT_CLASSIFICATION_PROMPT_TEMPLATE,
    EMERGENCY_RESPONSE_PROMPT_TEMPLATE,
    ASK_SITUATION_PROMPT_TEMPLATE,
    GOAL_OPTIONS_PROMPT_TEMPLATE,
    GROW_RESPONSE_PROMPT_TEMPLATE,
    RESEARCH_AGENT_PROMPT_TEMPLATE,
    EVALUATE_DOCS_PROMPT_TEMPLATE,
    PARSE_GOAL_SELECTION_PROMPT,
    get_baby_context_string,
    get_docs_context_string,
)
from app.agent.tools import milvus_knowledge_search, retrieve_qna
from app.services.qna_service import format_qna_docs
from app.dto.qna import QnADoc
from app.dto.rag import RagDoc
from app.core.llm_factory import get_generator_llm, get_evaluator_llm
from app.agent.utils import parse_json_from_response, track_node_execution_time
import logging

logger = logging.getLogger(__name__)

@track_node_execution_time("intent_classifier")
async def intent_classifier_node(state: AgentState) -> AgentState:
    """
    ì˜ë„ ë¶„ë¥˜ ë…¸ë“œ
    ì§ˆë¬¸ì´ 'ë¯¸ìˆ™ì•„ ëŒë´„' ë²”ìœ„ì¸ì§€ íŒë‹¨ + 'ë¶€ì¡±í•œ ì •ë³´ ì œê³µ' ì—¬ë¶€ íŒë‹¨
    """
    logger.info("===== ğŸ¤– ì˜ë„ ë¶„ë¥˜ ë…¸ë“œ ì‹¤í–‰ =====")
    
    question = state.get("question", "")
    
    llm = get_evaluator_llm()
    if not llm:
        logger.warning("í‰ê°€ ëª¨ë¸ ì—†ìŒ, ê¸°ë³¸ê°’(relevant) ì„¤ì •")
        state["_intent"] = "irrelevant"
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        return state
        
    try:

        messages = state.get("messages", [])
        recent_history = messages[-5:] if len(messages) > 5 else messages
        input_messages = [SystemMessage(content=INTENT_CLASSIFICATION_PROMPT_TEMPLATE)] + recent_history
        
        response = await llm.ainvoke(input_messages)
        response_text = response.content.strip()
        
        result = parse_json_from_response(response_text)
        
        intent = result.get("intent", "relevant")
        reason = result.get("reason", "")
        
        logger.info(f"âœ… ì˜ë„ ë¶„ë¥˜ ê²°ê³¼: {intent} (ì´ìœ : {reason}) âœ…")
        state["_intent"] = intent
        
        # irrelevantì¸ ê²½ìš° ì¦‰ì‹œ ë‹µë³€ ìƒì„±
        if intent == "irrelevant":
            logger.info("ğŸš« ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ -> ì¦‰ì‹œ ê±°ì ˆ ì‘ë‹µ ìƒì„± ğŸš«")
            try:
                simple_prompt = SIMPLE_RESPONSE_PROMPT_TEMPLATE.format(question=question)
                gen_llm = get_generator_llm()
                if gen_llm:

                    # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ íƒœê·¸ ì¶”ê°€
                    resp = await gen_llm.ainvoke(
                        [HumanMessage(content=simple_prompt)],
                        config={"tags": ["stream_response"]}
                    )
                    state["response"] = resp.content.strip()
                    state["messages"] = [resp]
                else:
                    state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¯¸ìˆ™ì•„ ë° ì‹ ìƒì•„ ëŒë´„ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            except Exception as ex:
                logger.error(f"ê±°ì ˆ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(ex)}")
                state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
    except Exception as e:
        logger.error(f"ì˜ë„ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        state["_intent"] = "relevant"
        
    return state

@track_node_execution_time("emergency_response")
async def emergency_response_node(state: AgentState) -> AgentState:
    """
    [í†µí•©] ì‘ê¸‰ ìƒí™© ì „ìš© ë…¸ë“œ (ê²€ìƒ‰ + ë‹µë³€ ìƒì„±)
    - ë³„ë„ì˜ í‰ê°€ ë…¸ë“œ ì—†ì´ ì¦‰ì‹œ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ì—¬ ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ë§Œ ì„ ë³„í•´ ë‹µë³€í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    """
    logger.info("===== ğŸš¨ Emergency Response ë…¸ë“œ ì‹¤í–‰ (Fast-Track) =====")
    
    question = state.get("question", "")
    baby_info = state.get("baby_info", {})
    state["is_emergency"] = True
    qna_docs = []
    rag_docs = []
    
    try:
        # 1. QnA ê²€ìƒ‰ (invoke ëŒ€ì‹  .func ì‚¬ìš©)
        # .funcë¥¼ í˜¸ì¶œí•˜ë©´ ë°ì½”ë ˆì´í„° í¬ì¥ì„ ë²—ê¸°ê³  (content, artifact) íŠœí”Œì„ ì§ì ‘ ë°›ìŠµë‹ˆë‹¤.
        qna_content, qna_artifacts = retrieve_qna.func(query=question)
        
        if qna_artifacts:
            for d in qna_artifacts:
                qna_docs.append(QnADoc(**d))
        
        # 2. Milvus ê²€ìƒ‰ (.func ì‚¬ìš©)
        # ì¸ìë¥¼ í‚¤ì›Œë“œ ì•„ê·œë¨¼íŠ¸(kwargs) í˜•íƒœë¡œ ëª…í™•íˆ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        milvus_content, milvus_artifacts = milvus_knowledge_search.func(query=question)
        
        if milvus_artifacts:
            for d in milvus_artifacts:
                rag_docs.append(RagDoc(**d))

        # ê²°ê³¼ ì €ì¥
        state["_qna_docs"] = qna_docs
        state["_retrieved_docs"] = rag_docs
        logger.info(f"ğŸš¨ ì‘ê¸‰ ê²€ìƒ‰ ì™„ë£Œ: {qna_content} {milvus_content}")
        
    except Exception as e:
        logger.error(f"ì‘ê¸‰ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œí•˜ê³  ì§„í–‰): {str(e)}")

    # 3. [ìƒì„±] ì‘ê¸‰ ë‹µë³€ ìƒì„±
    llm = get_generator_llm()
    if not llm:
        state["response"] = "ì‹œìŠ¤í…œ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì¦‰ì‹œ 119ì— ì—°ë½í•˜ê±°ë‚˜ ë³‘ì›ì„ ë°©ë¬¸í•˜ì„¸ìš”."
        return state
        
    try:
        baby_context = get_baby_context_string(baby_info)
        
        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        formatted_qna = format_qna_docs(qna_docs) if qna_docs else ""
        rag_context = get_docs_context_string(rag_docs)
        
        docs_context = ""
        if formatted_qna:
            docs_context += f"[QnA ì •ë³´]\n{formatted_qna}\n\n"
        if rag_context:
            docs_context += f"[ê²€ìƒ‰ëœ ë¬¸ì„œ]\n{rag_context}\n\n"
        if not docs_context:
            docs_context = "ê´€ë ¨ëœ ì°¸ì¡° ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì˜í•™ì  ìƒì‹ì— ê¸°ë°˜í•´ ë‹µë³€í•˜ì„¸ìš”."

        prompt = EMERGENCY_RESPONSE_PROMPT_TEMPLATE.format(
            baby_context=baby_context,
            docs_context=docs_context,
            question=question
        )
        
        # ìµœê·¼ ëŒ€í™” 5ê°œë§Œ ì°¸ì¡°
        messages = state.get("messages", [])
        clean_messages = get_clean_messages_for_generation(messages)
        recent_history = clean_messages[-5:] if len(clean_messages) > 5 else clean_messages
        
        response = await llm.ainvoke(
            [SystemMessage(content=prompt)] + recent_history,
            config={"tags": ["stream_response"]}
        )
        
        state["response"] = response.content.strip()
        state["messages"] = [response]
        
    except Exception as e:
        logger.error(f"ì‘ê¸‰ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ê°€ê¹Œìš´ ë³‘ì› ì‘ê¸‰ì‹¤ì„ ë°©ë¬¸í•˜ì„¸ìš”."
        
    return state


@track_node_execution_time("ask_situation")
async def ask_situation_node(state: AgentState) -> AgentState:
    """
    Ask Situation ë…¸ë“œ (1ë‹¨ê³„: í˜„ì¬ ìƒí™© ì§ˆë¬¸)
    - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, í˜„ì¬ ìƒí™©ì„ íŒŒì•…í•˜ëŠ” ê³µê°í˜• ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ì´ ë…¸ë“œì˜ ì¶œë ¥ì´ ìŠ¤íŠ¸ë¦¬ë°ë˜ì–´ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ë©ë‹ˆë‹¤.
    - ì´í›„ interruptë¡œ ì‚¬ìš©ìì˜ ìƒí™© ë‹µë³€ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
    """
    logger.info("===== ğŸ—£ï¸ Ask Situation ë…¸ë“œ ì‹¤í–‰ =====")
    
    question = state.get("question", "")
    baby_info = state.get("baby_info", {})
    
    llm = get_generator_llm()
    if not llm:
        default_msg = "ë” ì •í™•í•œ ë„ì›€ì„ ë“œë¦¬ê¸° ìœ„í•´, í˜„ì¬ ì•„ê¸°ì˜ ìƒíƒœë‚˜ ìƒí™©ì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        state["response"] = default_msg
        state["messages"] = [AIMessage(content=default_msg)]
        return state

    try:
        baby_context = get_baby_context_string(baby_info)
        
        system_prompt = ASK_SITUATION_PROMPT_TEMPLATE.format(
            question=question,
            baby_context=baby_context
        )
        
        messages = state.get("messages", [])
        recent_history = messages[-3:] if len(messages) > 3 else messages
        
        response = await llm.ainvoke(
            [SystemMessage(content=system_prompt)] + recent_history,
            config={"tags": ["stream_response"]}
        )
        
        state["response"] = response.content.strip()
        state["messages"] = [response]
        
        logger.info(f"âœ… ìƒí™© ì§ˆë¬¸ ìƒì„± ì™„ë£Œ: {state['response'][:30]}...")
        
    except Exception as e:
        logger.error(f"Ask Situation ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
        fallback_msg = "ë” ì •í™•í•œ ì¡°ì–¸ì„ ìœ„í•´ í˜„ì¬ ì•„ê¸° ìƒíƒœë¥¼ ìì„¸íˆ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?"
        state["response"] = fallback_msg
        state["messages"] = [AIMessage(content=fallback_msg)]
    
    return state


@track_node_execution_time("goal_options")
async def goal_options_node(state: AgentState) -> AgentState:
    """
    Goal Options ë…¸ë“œ (2ë‹¨ê³„: ëª©í‘œ ì„ íƒì§€ ì œì‹œ)
    - interruptë¡œ ë°›ì€ ì‚¬ìš©ìì˜ ìƒí™© ë‹µë³€ì„ í™œìš©í•˜ì—¬
    - ìµœì´ˆ ì§ˆë¬¸ + ìƒí™© ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ 2~3ê°œ ëª©í‘œ ì„ íƒì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    - ì´ ë…¸ë“œì˜ ì¶œë ¥ì´ ìŠ¤íŠ¸ë¦¬ë°ë˜ì–´ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ë©ë‹ˆë‹¤.
    - ì´í›„ interruptë¡œ ì‚¬ìš©ìì˜ ëª©í‘œ ì„ íƒì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
    """
    logger.info("===== ğŸ¯ Goal Options ë…¸ë“œ ì‹¤í–‰ =====")
    
    question = state.get("question", "")
    baby_info = state.get("baby_info", {})
    messages = state.get("messages", [])
    
    # 1. ì‚¬ìš©ì ìƒí™© ë‹µë³€ ìˆ˜ì§‘ (interrupt ì´í›„ ë§ˆì§€ë§‰ HumanMessage)
    user_situation = ""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_situation = msg.content
            break
    
    # user_current_infoì—ë„ ì €ì¥ (ì´í›„ ë‹¨ê³„ì—ì„œ í™œìš©)
    state["user_current_info"] = user_situation
    logger.info(f"ğŸ“ ì‚¬ìš©ì ìƒí™© ë‹µë³€: {user_situation[:50]}...")
    
    llm = get_generator_llm()
    if not llm:
        default_msg = "ì–´ë–¤ ë¶€ë¶„ì„ ê°€ì¥ ë¨¼ì € ë„ì™€ë“œë¦´ê¹Œìš”?\n1. í˜„ì¬ ìƒí™© ê°œì„ í•˜ê¸°\n2. ê´€ë ¨ ì •ë³´ ì•Œì•„ë³´ê¸°"
        state["response"] = default_msg
        state["messages"] = [AIMessage(content=default_msg)]
        state["goal_options"] = ["í˜„ì¬ ìƒí™© ê°œì„ í•˜ê¸°", "ê´€ë ¨ ì •ë³´ ì•Œì•„ë³´ê¸°"]
        return state

    try:
        baby_context = get_baby_context_string(baby_info)
        
        system_prompt = GOAL_OPTIONS_PROMPT_TEMPLATE.format(
            question=question,
            user_situation=user_situation,
            baby_context=baby_context
        )
        
        response = await llm.ainvoke(
            [SystemMessage(content=system_prompt)]
        )
        
        result = parse_json_from_response(response.content.strip())
        
        empathy = result.get("empathy", "")
        options = result.get("options", [])
        closing = result.get("closing", "ì–´ë–¤ ê±¸ ë¨¼ì € ë„ì™€ë“œë¦´ê¹Œìš”?")
        
        # ì„ íƒì§€ë¥¼ stateì— ì €ì¥
        state["goal_options"] = options
        
        # ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë©”ì‹œì§€ êµ¬ì„±
        display_msg = empathy + "\n\n"
        display_msg += "ì§€ê¸ˆ ê°€ì¥ í•´ê²°í•´ì£¼ê³  ì‹¶ì€ ê²Œ ì–´ë–¤ ê±´ê°€ìš”?\n\n"
        for i, option in enumerate(options, 1):
            display_msg += f"{i}. {option}\n"
        display_msg += f"\n{closing}"
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìœ¼ë¡œ ì „ë‹¬
        ai_msg = AIMessage(content=display_msg)
        state["response"] = display_msg
        state["messages"] = [ai_msg]
        
        logger.info(f"âœ… ëª©í‘œ ì„ íƒì§€ {len(options)}ê°œ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"Goal Options ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
        fallback_options = ["í˜„ì¬ ìƒí™© ê°œì„  ë°©ë²• ì•Œì•„ë³´ê¸°", "ê´€ë ¨ ì •ë³´ ìì„¸íˆ ì•Œì•„ë³´ê¸°"]
        fallback_msg = "ì–´ë–¤ ë¶€ë¶„ì´ ê°€ì¥ ê¶ê¸ˆí•˜ì„¸ìš”?\n\n1. í˜„ì¬ ìƒí™© ê°œì„  ë°©ë²• ì•Œì•„ë³´ê¸°\n2. ê´€ë ¨ ì •ë³´ ìì„¸íˆ ì•Œì•„ë³´ê¸°\n\në²ˆí˜¸ë¡œ ê³¨ë¼ì£¼ì‹œê±°ë‚˜, ì›í•˜ì‹œëŠ” ê²Œ ë”°ë¡œ ìˆìœ¼ë©´ ì§ì ‘ ì ì–´ì£¼ì…”ë„ ë¼ìš” ğŸ˜Š"
        state["response"] = fallback_msg
        state["messages"] = [AIMessage(content=fallback_msg)]
        state["goal_options"] = fallback_options
    
    return state


@track_node_execution_time("goal_selector")
async def goal_selector_node(state: AgentState) -> AgentState:
    """
    Goal Selector ë…¸ë“œ (ëª©í‘œ ì„ íƒ íŒŒì‹±)
    - interrupt ì´í›„ ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ëª©í‘œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    - Evaluator LLMì„ ì‚¬ìš©í•˜ì—¬ ë²ˆí˜¸ ì„ íƒ, ë³µìˆ˜ ì„ íƒ, ì»¤ìŠ¤í…€ ëª©í‘œë¥¼ ì •í™•í•˜ê²Œ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    logger.info("===== ğŸ¯ Goal Selector ë…¸ë“œ ì‹¤í–‰ =====")
    
    messages = state.get("messages", [])
    goal_options = state.get("goal_options", [])
    question = state.get("question", "")
    
    # 1. ì‚¬ìš©ìì˜ ëª©í‘œ ì„ íƒ ìˆ˜ì§‘ (ë‘ ë²ˆì§¸ interrupt ì´í›„ ë§ˆì§€ë§‰ HumanMessage)
    last_human_msg = ""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            last_human_msg = msg.content
            break
    
    # 2. Evaluator LLMìœ¼ë¡œ ìµœì¢… goal ê²°ì •
    selected_goal = last_human_msg  # ê¸°ë³¸ê°’: ì›ë¬¸ ê·¸ëŒ€ë¡œ
    is_relevant = True
    
    if goal_options and last_human_msg:
        try:
            eval_llm = get_evaluator_llm()
            if eval_llm:
                options_text = "\n".join(f"{i+1}. {opt}" for i, opt in enumerate(goal_options))
                
                parse_prompt = PARSE_GOAL_SELECTION_PROMPT.format(
                    options_text=options_text,
                    user_response=last_human_msg
                )
                
                parse_response = await eval_llm.ainvoke([SystemMessage(content=parse_prompt)])
                parse_result = parse_json_from_response(parse_response.content.strip())
                
                is_relevant = parse_result.get("is_relevant", True)
                parsed_goal = parse_result.get("goal")
                
                if is_relevant and parsed_goal:
                    selected_goal = parsed_goal
                    logger.info(f"ğŸ¯ LLM íŒŒì‹± ê²°ê³¼: {selected_goal}")
                elif not is_relevant:
                    logger.info(f"ğŸš« ê´€ë ¨ ì—†ëŠ” ì‘ë‹µ ê°ì§€: {last_human_msg[:30]}...")
                else:
                    logger.warning("âš ï¸ ëª©í‘œ íŒŒì‹± ê²°ê³¼ ì—†ìŒ, ì›ë¬¸ ì‚¬ìš©")
            else:
                logger.warning("âš ï¸ Evaluator LLM ì—†ìŒ, ì›ë¬¸ ì‚¬ìš©")
        except Exception as parse_err:
            logger.error(f"ëª©í‘œ ì„ íƒ íŒŒì‹± ì‹¤íŒ¨ (ì›ë¬¸ ì‚¬ìš©): {parse_err}")
    
    # 3. ê´€ë ¨ ì—†ëŠ” ì‘ë‹µì¸ ê²½ìš° â†’ ë˜ë¬»ê¸°
    if not is_relevant:
        retry_msg = "ì£„ì†¡í•˜ì§€ë§Œ ì§€ê¸ˆì€ ëª©í‘œë¥¼ ì„¤ì •í•˜ëŠ” ë‹¨ê³„ì˜ˆìš” ğŸ˜Š\n\n"
        # ê¸°ì¡´ ì„ íƒì§€ ë‹¤ì‹œ ë³´ì—¬ì£¼ê¸°
        for i, opt in enumerate(goal_options, 1):
            retry_msg += f"{i}. {opt}\n"
        retry_msg += "\në²ˆí˜¸ë¡œ ê³¨ë¼ì£¼ì‹œê±°ë‚˜, ì›í•˜ì‹œëŠ” ëª©í‘œë¥¼ ì§ì ‘ ì ì–´ì£¼ì„¸ìš”!"
        
        state["response"] = retry_msg
        state["messages"] = [AIMessage(content=retry_msg)]
        state["_goal_valid"] = False
        logger.info("ğŸ”„ ëª©í‘œ ì¬ì„ íƒ ìš”ì²­ (goal_selector self-loop)")
        return state
    
    state["goal"] = selected_goal
    state["_goal_valid"] = True
    logger.info(f"âœ… ìµœì¢… ì„¤ì • ëª©í‘œ: {selected_goal}")
    
    # user_current_infoê°€ ì—†ìœ¼ë©´ question ì‚¬ìš©
    if not state.get("user_current_info"):
        state["user_current_info"] = question
    
    return state


@track_node_execution_time("research_agent")
async def research_agent_node(state: AgentState) -> AgentState:
    """
    Research Agent ë…¸ë“œ (Tool Binding ì ìš©)
    - goal_selector_nodeì—ì„œ ì„¤ì •ëœ ëª©í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ
    - LLMì´ í•„ìš”í•œ ë„êµ¬(QnA, Milvus)ë¥¼ ì„ íƒí•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    logger.info("===== ğŸ•µï¸ Research Agent ë…¸ë“œ ì‹¤í–‰ =====")
    
    question = state.get("question", "")
    baby_info = state.get("baby_info", {})
    user_current_info = state.get("user_current_info", question)
    goal = state.get("goal", "")
    
    # 2. LLM + Tool Binding
    llm = get_generator_llm()
    if not llm:
        logger.error("LLM not found")
        return state

    # ì‚¬ìš©í•  ë„êµ¬ ë¦¬ìŠ¤íŠ¸
    tools = [retrieve_qna, milvus_knowledge_search]
    llm_with_tools = llm.bind_tools(tools)
    
    baby_context = get_baby_context_string(baby_info)
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = RESEARCH_AGENT_PROMPT_TEMPLATE.format(
        baby_context=baby_context,
        question=question,
        user_current_info=user_current_info,
        goal=goal,
    )
    
    try:
        # LLM í˜¸ì¶œ
        response = await llm_with_tools.ainvoke(
            [SystemMessage(content=system_prompt)],
            config={"tags": ["tool_selection"]}
        )
        
        qna_docs = []
        rag_docs = []
        
        # 3. ë„êµ¬ ì‹¤í–‰ (Manual Execution to capture artifacts)
        if response.tool_calls:
            logger.info(f"ğŸ› ï¸ ë„êµ¬ í˜¸ì¶œ ê°ì§€: {len(response.tool_calls)}ê°œ")
            
            for tool_call in response.tool_calls:
                name = tool_call["name"]
                args = tool_call["args"]
                logger.info(f"  -> Executing {name} with args: {args}")
                
                try:
                    if name == "retrieve_qna":
                        # .func()ë¥¼ ì‚¬ìš©í•˜ì—¬ contentì™€ artifacts(metadata)ë¥¼ ëª¨ë‘ ê°€ì ¸ì˜´
                        content, artifacts = retrieve_qna.func(**args)
                        if artifacts:
                            for d in artifacts:
                                qna_docs.append(QnADoc(**d))
                                
                    elif name == "milvus_knowledge_search":
                        content, artifacts = milvus_knowledge_search.func(**args)
                        if artifacts:
                            for d in artifacts:
                                rag_docs.append(RagDoc(**d))
                except Exception as tool_err:
                    logger.error(f"âŒ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ ({name}): {tool_err}")
                    
        else:
            logger.info("âš ï¸ ë„êµ¬ í˜¸ì¶œ ì—†ìŒ: LLMì´ ê²€ìƒ‰ì´ í•„ìš”ì—†ë‹¤ê³  íŒë‹¨í•˜ê±°ë‚˜ ì‹¤íŒ¨í•¨.")
            
        # ê²°ê³¼ ì €ì¥
        state["_qna_docs"] = qna_docs
        state["_retrieved_docs"] = rag_docs
        logger.info(f"âœ… Research ì™„ë£Œ: QnA {len(qna_docs)}ê°œ, Docs {len(rag_docs)}ê°œ")
        
    except Exception as e:
        logger.error(f"Research Agent ì‹¤íŒ¨: {e}", exc_info=True)
        
    return state


@track_node_execution_time("evaluate_docs")
async def evaluate_docs_node(state: AgentState) -> AgentState:
    """
    Evaluate Docs ë…¸ë“œ
    - ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ LLMìœ¼ë¡œ í‰ê°€í•˜ì—¬, ê´€ë ¨ ìˆëŠ” ë¬¸ì„œì˜ ì¸ë±ìŠ¤ë§Œ ì„ ë³„í•©ë‹ˆë‹¤.
    - ì„ ë³„ëœ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ì›ë³¸ ë¬¸ì„œë§Œ stateì— ë‚¨ê¹ë‹ˆë‹¤.
    """
    logger.info("===== ğŸ§ Evaluate Docs ë…¸ë“œ ì‹¤í–‰ =====")
    
    question = state.get("question", "")
    goal = state.get("goal", "")
    user_current_info = state.get("user_current_info", "")
    baby_info = state.get("baby_info", {})
    
    rag_docs = state.get("_retrieved_docs", [])
    qna_docs = state.get("_qna_docs", [])
    
    # 1. ë¬¸ì„œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë°”ë¡œ í†µê³¼
    if not rag_docs and not qna_docs:
        logger.info("â„¹ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ -> í‰ê°€ ìƒëµ")
        return state

    llm = get_evaluator_llm()
    if not llm:
        logger.warning("í‰ê°€ ëª¨ë¸ ì—†ìŒ -> ëª¨ë“  ë¬¸ì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
        return state

    try:
        baby_context = get_baby_context_string(baby_info)
        
        # 2. QnA ë¬¸ì„œ ëª©ë¡ í…ìŠ¤íŠ¸ ìƒì„± (ë²ˆí˜¸ í¬í•¨)
        qna_docs_list = "ì—†ìŒ"
        if qna_docs:
            lines = []
            for i, doc in enumerate(qna_docs):
                q = doc.get("question", "") if isinstance(doc, dict) else getattr(doc, "question", "")
                a = doc.get("answer", "") if isinstance(doc, dict) else getattr(doc, "answer", "")
                lines.append(f"[{i}] Q: {q}\n    A: {a[:200]}...")
            qna_docs_list = "\n".join(lines)
        
        # 3. RAG ë¬¸ì„œ ëª©ë¡ í…ìŠ¤íŠ¸ ìƒì„± (ë²ˆí˜¸ í¬í•¨)
        rag_docs_list = "ì—†ìŒ"
        if rag_docs:
            lines = []
            for i, doc in enumerate(rag_docs):
                content = doc.get("content", "") if isinstance(doc, dict) else getattr(doc, "content", "")
                filename = doc.get("filename", "N/A") if isinstance(doc, dict) else getattr(doc, "filename", "N/A")
                lines.append(f"[{i}] (ì¶œì²˜: {filename}) {content[:300]}...")
            rag_docs_list = "\n".join(lines)
        
        # 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° LLM í˜¸ì¶œ
        prompt = EVALUATE_DOCS_PROMPT_TEMPLATE.format(
            question=question,
            goal=goal,
            user_current_info=user_current_info,
            baby_context=baby_context,
            qna_docs_list=qna_docs_list,
            rag_docs_list=rag_docs_list
        )
        
        response = await llm.ainvoke([SystemMessage(content=prompt)])
        result = parse_json_from_response(response.content.strip())
        
        # 5. ì¸ë±ìŠ¤ ê¸°ë°˜ í•„í„°ë§
        relevant_qna_indices = result.get("relevant_qna_indices", [])
        relevant_rag_indices = result.get("relevant_rag_indices", [])
        reason = result.get("reason", "")
        
        # QnA í•„í„°ë§
        if qna_docs and relevant_qna_indices:
            filtered_qna = [qna_docs[i] for i in relevant_qna_indices if i < len(qna_docs)]
            state["_qna_docs"] = filtered_qna
            logger.info(f"ğŸ“‹ QnA í•„í„°ë§: {len(qna_docs)} -> {len(filtered_qna)}ê°œ")
        elif qna_docs and not relevant_qna_indices:
            state["_qna_docs"] = []
            logger.info(f"ğŸ“‹ QnA í•„í„°ë§: {len(qna_docs)} -> 0ê°œ (ê´€ë ¨ ì—†ìŒ)")
        
        # RAG í•„í„°ë§
        if rag_docs and relevant_rag_indices:
            filtered_rag = [rag_docs[i] for i in relevant_rag_indices if i < len(rag_docs)]
            state["_retrieved_docs"] = filtered_rag
            logger.info(f"ğŸ“„ RAG í•„í„°ë§: {len(rag_docs)} -> {len(filtered_rag)}ê°œ")
        elif rag_docs and not relevant_rag_indices:
            state["_retrieved_docs"] = []
            logger.info(f"ğŸ“„ RAG í•„í„°ë§: {len(rag_docs)} -> 0ê°œ (ê´€ë ¨ ì—†ìŒ)")
        
        logger.info(f"âœ… ë¬¸ì„œ í‰ê°€ ì™„ë£Œ (ì‚¬ìœ : {reason})")
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ í‰ê°€ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€
        
    return state


@track_node_execution_time("response_node")
async def grow_response_node(state: AgentState) -> AgentState:
    """
    Response Node (GROW ëª¨ë¸ ì ìš©)
    - ìˆ˜ì§‘ëœ ì •ë³´(Baby Info, User Reality, Goal, Docs)ë¥¼ ë°”íƒ•ìœ¼ë¡œ
    - GROW ëª¨ë¸ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    logger.info("===== ğŸŒ± GROW Response ë…¸ë“œ ì‹¤í–‰ =====")
    question = state.get("question", "")
    goal = state.get("goal", "")
    user_current_info = state.get("user_current_info", "")
    baby_info = state.get("baby_info", {})
    
    # í‰ê°€ ë…¸ë“œì—ì„œ í•„í„°ë§ëœ ì›ë³¸ ë¬¸ì„œë¥¼ ì‚¬ìš©
    rag_docs = state.get("_retrieved_docs", [])
    qna_docs = state.get("_qna_docs", [])
    
    # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    formatted_qna = format_qna_docs(qna_docs) if qna_docs else ""
    rag_context = get_docs_context_string(rag_docs)
    
    docs_context = ""
    if formatted_qna: docs_context += f"[QnA ì •ë³´]\n{formatted_qna}\n\n"
    if rag_context: docs_context += f"[ê²€ìƒ‰ëœ ë¬¸ì„œ]\n{rag_context}\n\n"
    if not docs_context: docs_context = "ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ (ì˜í•™ì  ìƒì‹ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€)"
    
    llm = get_generator_llm()
    if not llm:
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return state
        
    try:
        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì´ë¯¸ ìœ„ì—ì„œ docs_contextë¡œ ì¤€ë¹„ë¨)
        
        baby_context = get_baby_context_string(baby_info)
        
        # GROW í”„ë¡¬í”„íŠ¸ ì ìš©
        system_prompt = GROW_RESPONSE_PROMPT_TEMPLATE.format(
            baby_context=baby_context,
            user_current_info=user_current_info,
            docs_context=docs_context,
            goal=goal,
            question=question
        )
        
        messages = state.get("messages", [])
        clean_messages = get_clean_messages_for_generation(messages)
        recent_history = clean_messages[-5:] # ìµœê·¼ ëŒ€í™” ì¼ë¶€ í¬í•¨
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + íˆìŠ¤í† ë¦¬ -> ë‹µë³€ ìƒì„±
        response = await llm.ainvoke(
            [SystemMessage(content=system_prompt)] + recent_history,
            config={"tags": ["stream_response"]}
        )
        
        state["response"] = response.content.strip()
        # ë‹µë³€ì„ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¡œ ì¶”ê°€
        state["messages"] = [response]
        
        logger.info("âœ… GROW ë‹µë³€ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"GROW ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        state["messages"] = [AIMessage(content=state["response"])]
        
    return state


def get_clean_messages_for_generation(messages):
    """
    ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ ìµœê·¼ HumanMessageê¹Œì§€ë§Œ ë‚¨ê¸°ê³  ê·¸ ì´í›„ì˜ Agent í™œë™ ë¡œê·¸ëŠ” ì œê±°
    
    Args:
        messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì •ë¦¬ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    """
    if not messages:
        return []
    
    # 1. ë’¤ì—ì„œë¶€í„° íƒìƒ‰í•˜ì—¬ 'ê°€ì¥ ìµœê·¼ì˜ HumanMessage' ì¸ë±ìŠ¤ ì°¾ê¸°
    last_human_index = -1
    for i, msg in enumerate(reversed(messages)):
        if isinstance(msg, HumanMessage):
            # reversed ìƒíƒœì´ë¯€ë¡œ ì›ë˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
            last_human_index = len(messages) - 1 - i
            break
            
    # 2. HumanMessageê°€ ì—†ë‹¤ë©´? (ì˜ˆì™¸ì²˜ë¦¬)
    if last_human_index == -1:
        return messages[-10:]  # ê·¸ëƒ¥ ìµœê·¼êº¼ ë°˜í™˜
        
    # 3. [í•µì‹¬] ë§ˆì§€ë§‰ ì§ˆë¬¸ê¹Œì§€ë§Œ ë‚¨ê¸°ê³ , ê·¸ ë’¤ì˜ Agent í™œë™ ë¡œê·¸ëŠ” ì „ë¶€ ì‚­ì œ
    clean_history = messages[:last_human_index + 1]
    
    return clean_history



