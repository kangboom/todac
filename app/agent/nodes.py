"""
ë…¸ë“œ í•¨ìˆ˜ (Self-RAG êµ¬ì¡°)
"""
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AIMessage
from app.agent.state import AgentState
from app.agent.prompts import (
    DOC_RELEVANCE_PROMPT_TEMPLATE, 
    RESPONSE_GENERATION_PROMPT_TEMPLATE,
    AGENT_NODE_PROMPT_TEMPLATE,
    get_baby_context_string,
    get_docs_context_string,
    SIMPLE_RESPONSE_PROMPT_TEMPLATE,
    INTENT_CLASSIFICATION_PROMPT_TEMPLATE,
    ASK_FOR_INFO_PROMPT_TEMPLATE,
    EMERGENCY_RESPONSE_PROMPT_TEMPLATE
)
from app.agent.tools import milvus_knowledge_search, retrieve_qna
from app.services.qna_service import format_qna_docs
from app.dto.qna import QnADoc
from app.dto.rag import RagDoc
from app.core.llm_factory import get_generator_llm, get_evaluator_llm
from app.agent.utils import parse_json_from_response, track_node_execution_time
import logging

logger = logging.getLogger(__name__)

@track_node_execution_time("intent_classifier")
async def intent_classifier_node(state: AgentState) -> AgentState:
    """
    ì˜ë„ ë¶„ë¥˜ ë…¸ë“œ
    ì§ˆë¬¸ì´ 'ë¯¸ìˆ™ì•„ ëŒë´„' ë²”ìœ„ì¸ì§€ íŒë‹¨ + 'ë¶€ì¡±í•œ ì •ë³´ ì œê³µ' ì—¬ë¶€ íŒë‹¨
    """
    logger.info("===== ğŸ¤– ì˜ë„ ë¶„ë¥˜ ë…¸ë“œ ì‹¤í–‰ =====")
    
    # missing_info ìˆìœ¼ë©´ provide_missing_infoë¡œ ì„¤ì •
    missing_info_data = state.get("_missing_info")
    missing_info = missing_info_data.get("missing_info", []) if missing_info_data else []
        
    if missing_info:
        state["_intent"] = "provide_missing_info"
        return state
    
    question = state.get("question", "")
    
    llm = get_evaluator_llm()
    if not llm:
        logger.warning("í‰ê°€ ëª¨ë¸ ì—†ìŒ, ê¸°ë³¸ê°’(relevant) ì„¤ì •")
        state["_intent"] = "irrelevant"
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        return state
        
    try:

        messages = state.get("messages", [])
        recent_history = messages[-5:] if len(messages) > 5 else messages
        input_messages = [SystemMessage(content=INTENT_CLASSIFICATION_PROMPT_TEMPLATE)] + recent_history
        
        response = await llm.ainvoke(input_messages)
        response_text = response.content.strip()
        
        result = parse_json_from_response(response_text)
        
        intent = result.get("intent", "relevant")
        reason = result.get("reason", "")
        
        logger.info(f"âœ… ì˜ë„ ë¶„ë¥˜ ê²°ê³¼: {intent} (ì´ìœ : {reason}) âœ…")
        state["_intent"] = intent
        
        # irrelevantì¸ ê²½ìš° ì¦‰ì‹œ ë‹µë³€ ìƒì„±
        if intent == "irrelevant":
            logger.info("ğŸš« ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ -> ì¦‰ì‹œ ê±°ì ˆ ì‘ë‹µ ìƒì„± ğŸš«")
            try:
                simple_prompt = SIMPLE_RESPONSE_PROMPT_TEMPLATE.format(question=question)
                gen_llm = get_generator_llm()
                if gen_llm:

                    # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ íƒœê·¸ ì¶”ê°€
                    resp = await gen_llm.ainvoke(
                        [HumanMessage(content=simple_prompt)],
                        config={"tags": ["stream_response"]}
                    )
                    state["response"] = resp.content.strip()
                    state["messages"] = [resp]
                else:
                    state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¯¸ìˆ™ì•„ ë° ì‹ ìƒì•„ ëŒë´„ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            except Exception as ex:
                logger.error(f"ê±°ì ˆ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(ex)}")
                state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
    except Exception as e:
        logger.error(f"ì˜ë„ ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
        state["_intent"] = "relevant"
        
    return state

@track_node_execution_time("emergency_response")
async def emergency_response_node(state: AgentState) -> AgentState:
    """
    [í†µí•©] ì‘ê¸‰ ìƒí™© ì „ìš© ë…¸ë“œ (ê²€ìƒ‰ + ë‹µë³€ ìƒì„±)
    - ë³„ë„ì˜ í‰ê°€ ë…¸ë“œ ì—†ì´ ì¦‰ì‹œ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ì—¬ ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ë§Œ ì„ ë³„í•´ ë‹µë³€í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    """
    logger.info("===== ğŸš¨ Emergency Response ë…¸ë“œ ì‹¤í–‰ (Fast-Track) =====")
    
    question = state.get("question", "")
    baby_info = state.get("baby_info", {})
    
    qna_docs = []
    rag_docs = []
    
    try:
        # 1. QnA ê²€ìƒ‰ (invoke ëŒ€ì‹  .func ì‚¬ìš©)
        # .funcë¥¼ í˜¸ì¶œí•˜ë©´ ë°ì½”ë ˆì´í„° í¬ì¥ì„ ë²—ê¸°ê³  (content, artifact) íŠœí”Œì„ ì§ì ‘ ë°›ìŠµë‹ˆë‹¤.
        qna_content, qna_artifacts = retrieve_qna.func(query=question)
        
        if qna_artifacts:
            for d in qna_artifacts:
                qna_docs.append(QnADoc(**d))
        
        # 2. Milvus ê²€ìƒ‰ (.func ì‚¬ìš©)
        # ì¸ìë¥¼ í‚¤ì›Œë“œ ì•„ê·œë¨¼íŠ¸(kwargs) í˜•íƒœë¡œ ëª…í™•íˆ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        milvus_content, milvus_artifacts = milvus_knowledge_search.func(query=question)
        
        if milvus_artifacts:
            for d in milvus_artifacts:
                rag_docs.append(RagDoc(**d))

        # ê²°ê³¼ ì €ì¥
        state["_qna_docs"] = qna_docs
        state["_retrieved_docs"] = rag_docs
        logger.info(f"ğŸš¨ ì‘ê¸‰ ê²€ìƒ‰ ì™„ë£Œ: {qna_content} {milvus_content}")
        
    except Exception as e:
        logger.error(f"ì‘ê¸‰ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œí•˜ê³  ì§„í–‰): {str(e)}")

    # 3. [ìƒì„±] ì‘ê¸‰ ë‹µë³€ ìƒì„±
    llm = get_generator_llm()
    if not llm:
        state["response"] = "ì‹œìŠ¤í…œ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì¦‰ì‹œ 119ì— ì—°ë½í•˜ê±°ë‚˜ ë³‘ì›ì„ ë°©ë¬¸í•˜ì„¸ìš”."
        return state
        
    try:
        baby_context = get_baby_context_string(baby_info)
        
        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        formatted_qna = format_qna_docs(qna_docs) if qna_docs else ""
        rag_context = get_docs_context_string(rag_docs)
        
        docs_context = ""
        if formatted_qna:
            docs_context += f"[QnA ì •ë³´]\n{formatted_qna}\n\n"
        if rag_context:
            docs_context += f"[ê²€ìƒ‰ëœ ë¬¸ì„œ]\n{rag_context}\n\n"
        if not docs_context:
            docs_context = "ê´€ë ¨ëœ ì°¸ì¡° ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì˜í•™ì  ìƒì‹ì— ê¸°ë°˜í•´ ë‹µë³€í•˜ì„¸ìš”."

        prompt = EMERGENCY_RESPONSE_PROMPT_TEMPLATE.format(
            baby_context=baby_context,
            docs_context=docs_context,
            question=question
        )
        
        # ìµœê·¼ ëŒ€í™” 5ê°œë§Œ ì°¸ì¡°
        messages = state.get("messages", [])
        clean_messages = get_clean_messages_for_generation(messages)
        recent_history = clean_messages[-5:] if len(clean_messages) > 5 else clean_messages
        
        response = await llm.ainvoke(
            [SystemMessage(content=prompt)] + recent_history,
            config={"tags": ["stream_response"]}
        )
        
        state["response"] = response.content.strip()
        state["messages"] = [response]
        
    except Exception as e:
        logger.error(f"ì‘ê¸‰ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ê°€ê¹Œìš´ ë³‘ì› ì‘ê¸‰ì‹¤ì„ ë°©ë¬¸í•˜ì„¸ìš”."
        
    return state


@track_node_execution_time("agent")
async def agent_node(state: AgentState) -> AgentState:
    """
    í•µì‹¬ ì—ì´ì „íŠ¸ ë…¸ë“œ (Self-RAG)
    - ì§ˆë¬¸ ë¶„ì„ ë° tool í˜¸ì¶œ ê²°ì •
    - Tool í˜¸ì¶œì´ í•„ìš”í•˜ë©´ tool í˜¸ì¶œ
    - ì´ì „ ë‹¨ê³„ì˜ Tool ì‹¤í–‰ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ì—¬ State ì—…ë°ì´íŠ¸
    """
    logger.info("===== ğŸ¤– Agent ë…¸ë“œ ì‹¤í–‰ =====")

    # 1. ToolMessage ì²˜ë¦¬ ë° State ì—…ë°ì´íŠ¸
    messages = state.get("messages", [])
    new_retrieved_docs = []
    new_qna_docs = []
    
    # ë©”ì‹œì§€ë¥¼ ì—­ìˆœìœ¼ë¡œ í™•ì¸í•˜ë©° ê°€ì¥ ìµœê·¼ì˜ ToolMessageë“¤ì„ ë¶„ì„
    for msg in reversed(messages):
        # HumanMessageê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì˜ ToolMessageë“¤ë§Œ ìœ íš¨
        if isinstance(msg, HumanMessage):
            break
        if isinstance(msg, AIMessage):
            continue 
            
        if isinstance(msg, ToolMessage):
            tool_name = getattr(msg, "name", "")
            raw_data = getattr(msg, "artifact", None)
            
            if not raw_data:
                continue # artifactê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ (í˜¹ì€ ì—ëŸ¬ì²˜ë¦¬)

            logger.info(f"ğŸ” ToolMessage Artifact ì¶”ì¶œ: {tool_name}")

            if tool_name == "milvus_knowledge_search":
                # raw_dataê°€ ì´ë¯¸ ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ ê°ì²´ì´ë¯€ë¡œ ë°”ë¡œ ìˆœíšŒ
                for d in raw_data:
                    try:
                        new_retrieved_docs.append(RagDoc(**d))
                    except Exception as e:
                        logger.error(f"RagDoc ë³€í™˜ ì‹¤íŒ¨: {e}")
            
            elif tool_name == "retrieve_qna":
                for d in raw_data:
                    try:
                        new_qna_docs.append(QnADoc(**d))
                    except Exception as e:
                        logger.error(f"QnADoc ë³€í™˜ ì‹¤íŒ¨: {e}")

    # State ì—…ë°ì´íŠ¸
    if new_retrieved_docs:
        state["_retrieved_docs"] = new_retrieved_docs
        logger.info(f"âœ… RAG ë¬¸ì„œ State ì—…ë°ì´íŠ¸: {len(new_retrieved_docs)}ê°œ")
        
    if new_qna_docs:
        state["_qna_docs"] = new_qna_docs
        logger.info(f"âœ… QnA ë¬¸ì„œ State ì—…ë°ì´íŠ¸: {len(new_qna_docs)}ê°œ")

    # 2. Agent ì‹¤í–‰ (LLM í˜¸ì¶œ)
    question = state.get("question", "")
    baby_info = state.get("baby_info", {})
    
    llm = get_generator_llm()
    if not llm:
        logger.error("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        return state
    
    try:
        # bind_tools ì‚¬ìš©í•˜ì—¬ íˆ´ ë°”ì¸ë”©
        tools = [
            milvus_knowledge_search,  # RAG ê²€ìƒ‰ tool
            retrieve_qna,             # QnA ê²€ìƒ‰ tool
        ]
        model_with_tools = llm.bind_tools(tools)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì•„ê¸° ì •ë³´ í¬í•¨)
        baby_context = get_baby_context_string(baby_info)
        
        # [ìˆ˜ì •] system_prompt ì¸ì ì œê±° (í…œí”Œë¦¿ì— í†µí•©ë¨)
        system_prompt = AGENT_NODE_PROMPT_TEMPLATE.format(
            baby_context=baby_context,
            question=question
        )
        
        recent_history = messages[-5:] if len(messages) > 5 else messages
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
        messages_with_system = [SystemMessage(content=system_prompt)] + recent_history
        
        # Agent ì‹¤í–‰
        response = await model_with_tools.ainvoke(messages_with_system)
        # ì‘ë‹µì„ ë©”ì‹œì§€ì— ì¶”ê°€
        state["messages"] = [response]
        
    except Exception as e:
        logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    return state


@track_node_execution_time("evaluate")
async def evaluate_node(state: AgentState) -> AgentState:
    """
    Grade Documents Node (Self-RAG)
    ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì§ˆë¬¸ ê´€ë ¨ì„±ì„ í‰ê°€
    """
    logger.info("===== ğŸ¤– í‰ê°€ ë…¸ë“œ ì‹¤í–‰ =====")
    question = state.get("question") or state.get("previous_question")
    
    retrieved_docs = state.get("_retrieved_docs", [])
    qna_docs = state.get("_qna_docs", []) or []

    if not retrieved_docs and not qna_docs:
        logger.warning("âš ï¸ í‰ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (RAG ë° QnA ëª¨ë‘ ì—†ìŒ).")
        state["_doc_relevance_score"] = 0.0
        state["_doc_relevance_passed"] = False
        return state
    
    llm = get_evaluator_llm()
    if not llm:
        logger.warning("âš ï¸ í‰ê°€ ëª¨ë¸ì´ ì—†ì–´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        state["_doc_relevance_score"] = 0.5
        state["_doc_relevance_passed"] = True
        return state
    
    try:
        # í‰ê°€ ëŒ€ìƒ êµ¬ì„±
        rag_to_evaluate = retrieved_docs[:5]
        qna_to_evaluate = qna_docs[:3]
        
        docs_summary = ""
        current_idx = 1
        
        # RAG ë¬¸ì„œ ìš”ì•½ ì¶”ê°€
        for doc in rag_to_evaluate:
            content = getattr(doc, "content", "")
            docs_summary += f"\në¬¸ì„œ {current_idx} (ì¼ë°˜ ë¬¸ì„œ):\n{content}\n"
            current_idx += 1
            
        # QnA ë¬¸ì„œ ìš”ì•½ ì¶”ê°€
        for doc in qna_to_evaluate:
            # Pydantic ëª¨ë¸ ì ‘ê·¼
            q = getattr(doc, "question", "")
            a = getattr(doc, "answer", "")
            docs_summary += f"\në¬¸ì„œ {current_idx} (QnA):\nQ: {q}\nA: {a}\n"
            current_idx += 1
        
        baby_context = get_baby_context_string(state.get("baby_info", {}))
        evaluation_prompt = DOC_RELEVANCE_PROMPT_TEMPLATE.format(
            baby_context=baby_context,
            question=question,
            docs_summary=docs_summary
        )
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ì •í™•í•˜ê²Œ í‰ê°€í•˜ì„¸ìš”."),
            HumanMessage(content=evaluation_prompt)
        ]
        
        response = await llm.ainvoke(messages)
        response_text = response.content.strip()
        
        evaluation_result = parse_json_from_response(response_text)
        
        score = float(evaluation_result.get("score", 0.5))
        
        # ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œ ì¸ë±ìŠ¤ ì¶”ì¶œ (1-based index)
        relevant_indices = evaluation_result.get("relevant_indices", [])
        
        # [ì¶”ê°€] ë¶€ì¡±í•œ ì •ë³´ ì¶”ì¶œ
        missing_info_list = evaluation_result.get("missing_info", [])

        state["_doc_relevance_score"] = max(0.0, min(1.0, score))
        state["_doc_relevance_passed"] = score >= 0.7
        
        # missing_infoê°€ ìˆìœ¼ë©´ Stateì— ì €ì¥ (ì´ë²ˆ í„´ ê¸°ì¤€ ë®ì–´ì“°ê¸°)
        if missing_info_list:
            state["_missing_info"] = {
                "missing_info": missing_info_list,
                "reason": evaluation_result.get("reason", ""),
            }
            logger.info(f"ğŸ” ë¶€ì¡±í•œ ì •ë³´ ì‹ë³„ë¨: {missing_info_list}")

        logger.info(f"âœ… ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼: {evaluation_result.get('reason', '')}")
        logger.info(f"âœ… ì ìˆ˜={score:.2f}, í†µê³¼={state['_doc_relevance_passed']}")
        
        # [ìˆ˜ì •] RAGì™€ QnA ë¶„ë¦¬í•˜ì—¬ í•„í„°ë§
        filtered_rag = []
        filtered_qna = []
        
        rag_count = len(rag_to_evaluate)
        
        if relevant_indices:
            for idx in relevant_indices:
                # 1-based index -> 0-based
                real_idx = idx - 1 
                
                if real_idx < rag_count:
                    # RAG ë¬¸ì„œ ë²”ìœ„
                    filtered_rag.append(rag_to_evaluate[real_idx])
                else:
                    # QnA ë¬¸ì„œ ë²”ìœ„
                    qna_idx = real_idx - rag_count
                    if qna_idx < len(qna_to_evaluate):
                        filtered_qna.append(qna_to_evaluate[qna_idx])
        
        # í•„í„°ë§ ê²°ê³¼ ì ìš©
        logger.info(f"âœ… ê´€ë ¨ì„± í•„í„°ë§ (RAG): {len(retrieved_docs)} -> {len(filtered_rag)}")
        logger.info(f"âœ… ê´€ë ¨ì„± í•„í„°ë§ (QnA): {len(qna_docs)} -> {len(filtered_qna)}")
        
        state["_retrieved_docs"] = filtered_rag
        state["_qna_docs"] = filtered_qna # í•„í„°ë§ëœ QnAë¡œ êµì²´
    
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ í‰ê°€ ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["_doc_relevance_score"] = 0.5
        state["_doc_relevance_passed"] = True
    
    return state

@track_node_execution_time("generate")
async def generate_node(state: AgentState) -> AgentState:
    """
    Generate Node (Self-RAG)
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± ë˜ëŠ” ë¶€ì¡±í•œ ì •ë³´ ìš”ì²­
    """
    logger.info("--- ğŸ¤– ë‹µë³€ ìƒì„± ë…¸ë“œ ì‹¤í–‰ ---")
    question = state.get("question") or state.get("previous_question", "")
    baby_info = state.get("baby_info", {})
    messages = state.get("messages", [])
    
    missing_info_data = state.get("_missing_info")
    is_doc_passed = state.get("_doc_relevance_passed", True)
    
    llm = get_generator_llm()
    if not llm:
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return state

    prompt = ""
    
    # 1. ì •ë³´ ë¶€ì¡± ì‹œ ì§ˆë¬¸ ìƒì„± ëª¨ë“œ
    if not is_doc_passed and isinstance(missing_info_data, dict):
        logger.info("ğŸ“ ì •ë³´ ë¶€ì¡± ì‹œ ì§ˆë¬¸ ìƒì„± ëª¨ë“œ(Relevance Failed)")
        missing_info_list = missing_info_data.get("missing_info", [])
        reason = missing_info_data.get("reason", "")
        
        if missing_info_list:
            baby_context = get_baby_context_string(baby_info)
            missing_info_str = ", ".join(missing_info_list)
            
            prompt = ASK_FOR_INFO_PROMPT_TEMPLATE.format(
                baby_context=baby_context,
                question=question,
                missing_info=missing_info_str,
                reason=reason
            )
        else:
             logger.warning("missing_info ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ ì¼ë°˜ ë‹µë³€ ëª¨ë“œë¡œ ì „í™˜")

    # 2. ì¼ë°˜ ë‹µë³€ ìƒì„± ëª¨ë“œ (ì •ë³´ ë¶€ì¡± ëª¨ë“œê°€ ì•„ë‹ ë•Œ)
    if is_doc_passed:
        logger.info("ğŸ“ ì¼ë°˜ ë‹µë³€ ìƒì„± ëª¨ë“œ(Relevance Passed)")
        retrieved_docs = state.get("_retrieved_docs", [])
        qna_docs = state.get("_qna_docs", [])
        
        # missing_info ë¬¸ìì—´ ìƒì„±
        missing_info_str = "ì—†ìŒ"
        if missing_info_data and isinstance(missing_info_data, dict):
             m_list = missing_info_data.get("missing_info", [])
             if m_list:
                 missing_info_str = ", ".join(m_list)
        
        baby_context = get_baby_context_string(baby_info)
        
        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        formatted_qna = format_qna_docs(qna_docs) if qna_docs else ""
        rag_context = get_docs_context_string(retrieved_docs)
        
        docs_context = ""
        if formatted_qna:
            docs_context += f"[QnA ì •ë³´]\n{formatted_qna}\n\n"
        if rag_context:
            docs_context += f"[ê²€ìƒ‰ëœ ë¬¸ì„œ]\n{rag_context}\n\n"
        
        if not docs_context:
            docs_context = "ê´€ë ¨ëœ ì°¸ì¡° ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì „ë¬¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        prompt = RESPONSE_GENERATION_PROMPT_TEMPLATE.format(
            baby_context=baby_context,
            docs_context=docs_context,
            missing_info=missing_info_str
        )
        
        # ì¼ë°˜ ëª¨ë“œì—ì„œëŠ” ë‹µë³€ ìƒì„± í›„ missing_info ì´ˆê¸°í™”
        state["_missing_info"] = None

    # 3. ê³µí†µ LLM í˜¸ì¶œ
    try:
        clean_messages = get_clean_messages_for_generation(messages)
        
        # ìµœê·¼ Nê°œë§Œ ì°¸ì¡° (í† í° ì œí•œ)
        recent_history = clean_messages[-5:] if len(clean_messages) > 5 else clean_messages
            
        response = await llm.ainvoke(
            [SystemMessage(content=prompt)] + recent_history,
            config={"tags": ["stream_response"]}
        )
        
        state["response"] = response.content.strip()
        state["messages"] = [response]
        
    except Exception as e:
        logger.error(f"ë‹µë³€/ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
        state["response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        state["_missing_info"] = None
    
    return state

def get_clean_messages_for_generation(messages):
    """
    ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ ìµœê·¼ HumanMessageê¹Œì§€ë§Œ ë‚¨ê¸°ê³  ê·¸ ì´í›„ì˜ Agent í™œë™ ë¡œê·¸ëŠ” ì œê±°
    
    Args:
        messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì •ë¦¬ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    """
    if not messages:
        return []
    
    # 1. ë’¤ì—ì„œë¶€í„° íƒìƒ‰í•˜ì—¬ 'ê°€ì¥ ìµœê·¼ì˜ HumanMessage' ì¸ë±ìŠ¤ ì°¾ê¸°
    last_human_index = -1
    for i, msg in enumerate(reversed(messages)):
        if isinstance(msg, HumanMessage):
            # reversed ìƒíƒœì´ë¯€ë¡œ ì›ë˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
            last_human_index = len(messages) - 1 - i
            break
            
    # 2. HumanMessageê°€ ì—†ë‹¤ë©´? (ì˜ˆì™¸ì²˜ë¦¬)
    if last_human_index == -1:
        return messages[-10:]  # ê·¸ëƒ¥ ìµœê·¼êº¼ ë°˜í™˜
        
    # 3. [í•µì‹¬] ë§ˆì§€ë§‰ ì§ˆë¬¸ê¹Œì§€ë§Œ ë‚¨ê¸°ê³ , ê·¸ ë’¤ì˜ Agent í™œë™ ë¡œê·¸ëŠ” ì „ë¶€ ì‚­ì œ
    clean_history = messages[:last_human_index + 1]
    
    return clean_history
