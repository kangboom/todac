"""
TaskIQ Worker Tasks
ë¬¸ì„œ ì²˜ë¦¬ ë“± ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì •ì˜
"""
import uuid
import logging
import json
import boto3
from typing import List
from app.core.taskiq import broker
from app.core.database import SessionLocal
from app.models.knowledge import KnowledgeDoc
from app.services.parsers.llama_parse_parser import LlamaParseParser
from app.services.parsers.pymupdf_parser import PyMuPDFParser
from app.services.parsers.docling_parser import DoclingParser
from app.services.chunking_markdown import chunk_markdown_documents
from app.services.markdown_service import cleanup_markdown_with_llm
from app.services.parser_service import get_parser
from app.services.s3_service import upload_to_s3, delete_from_s3, generate_storage_paths
from app.agent.tools import get_embedding
from app.core.milvus_schema import get_milvus_collection_safe
from app.core.config import settings

logger = logging.getLogger(__name__)

# S3 í´ë¼ì´ì–¸íŠ¸ (boto3)
s3_client = boto3.client(
    's3',
    aws_access_key_id=settings.AWS_ACCESS_KEY_ID,
    aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,
    region_name=settings.S3_REGION
)

@broker.task
async def process_document_task(
    doc_id_str: str,
    raw_s3_key: str,
    filename: str,
    category: str,
    user_id_str: str,
    file_size: int,
    doc_hash: str
):
    """
    ë°±ê·¸ë¼ìš´ë“œ ë¬¸ì„œ ì²˜ë¦¬ íƒœìŠ¤í¬
    1. S3ì—ì„œ ì›ë³¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    2. ë¬¸ì„œ íŒŒì‹± & Markdown ë³€í™˜
    3. Markdown S3 ì—…ë¡œë“œ
    4. ì²­í‚¹ & ì„ë² ë”©
    5. Milvus ì €ì¥
    6. DB ë©”íƒ€ë°ì´í„° ì €ì¥
    """
    logger.info(f"ğŸš€ ë¬¸ì„œ ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹œì‘: doc_id={doc_id_str}, file={filename}")
    
    db = SessionLocal()
    doc_id = uuid.UUID(doc_id_str)
    user_id = uuid.UUID(user_id_str)
    
    # ë¡¤ë°±ìš© ë¦¬ì†ŒìŠ¤ ì¶”ì 
    uploaded_s3_keys = []
    milvus_inserted = False
    
    try:
        # 1. S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        try:
            response = s3_client.get_object(Bucket=settings.S3_BUCKET_NAME, Key=raw_s3_key)
            content = response['Body'].read()
            logger.info(f"S3 ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(content)} bytes")
        except Exception as e:
            logger.error(f"S3 íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e

        # 2. íŒŒì„œ ì°¾ê¸°
        parser = get_parser(filename)
        if not parser:
            logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {filename}")
            return # ì‹¤íŒ¨ ì²˜ë¦¬ (DB ì—…ë°ì´íŠ¸ ë“± í•„ìš”í•  ìˆ˜ ìˆìŒ)

        # 3. ë¬¸ì„œ íŒŒì‹±
        try:
            documents = parser.parse(content, filename)
        except Exception as e:
            logger.error(f"ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise e
            
        # 3-1. Markdown ë³´ì •
        if isinstance(parser, (LlamaParseParser, PyMuPDFParser, DoclingParser)):
            if documents and len(documents) > 0:
                original_text = documents[0].text
                if original_text:
                    cleaned_text = cleanup_markdown_with_llm(original_text, filename)
                    documents[0].text = cleaned_text
                    logger.info(f"Markdown ë³´ì • ì™„ë£Œ: {filename}")

        # 4. í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = chunk_markdown_documents(documents)
        if not chunks:
            raise ValueError("íŒŒì‹±ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # 5. Markdown í…ìŠ¤íŠ¸ S3 ì—…ë¡œë“œ (Processed)
        markdown_text = documents[0].text if documents else ""
        markdown_bytes = markdown_text.encode('utf-8')
        
        storage_paths = generate_storage_paths(doc_id, filename)
        
        # processed_md_keyë¡œ ì—…ë¡œë“œ (raw_s3_keyëŠ” ì´ë¯¸ API ì„œë²„ì—ì„œ ì˜¬ë¦¼)
        storage_url = upload_to_s3(
            content=markdown_bytes,
            s3_key=storage_paths.processed_md_key,
            content_type='text/markdown'
        )
        uploaded_s3_keys.append(storage_url)

        # 6. ì„ë² ë”© ë° Milvus ì €ì¥
        try:
            collection = get_milvus_collection_safe()
            milvus_data = []
            
            for chunk in chunks:
                header_metadata = {
                    k: v for k, v in chunk.metadata.items() 
                    if k.startswith("Header")
                }
                
                if header_metadata:
                    sorted_headers = [header_metadata[k] for k in sorted(header_metadata.keys())]
                    header_path = " > ".join(sorted_headers)
                    embedding_text = f"{header_path}\n\n{chunk.text}"
                else:
                    embedding_text = chunk.text
                
                embedding = get_embedding(embedding_text)
                
                headers_json = json.dumps(header_metadata, ensure_ascii=False) if header_metadata else "{}"
                
                milvus_data.append({
                    "doc_id": str(doc_id),
                    "chunk_index": chunk.chunk_index,
                    "embedding": embedding,
                    "content": chunk.text[:65535],
                    "filename": filename[:255],
                    "category": category[:50],
                    "headers": headers_json[:2048]
                })
            
            collection.insert(milvus_data)
            collection.flush()
            milvus_inserted = True
            logger.info(f"Milvus ì €ì¥ ì™„ë£Œ: {len(milvus_data)}ê°œ ì²­í¬")
            
        except Exception as e:
            logger.error(f"Milvus ì €ì¥ ì‹¤íŒ¨: {e}")
            raise e

        # 7. DB ì €ì¥ (KnowledgeDoc)
        # ì´ë¯¸ APIì—ì„œ raw_pdf_url ë“±ì„ ì•Œê³  ìˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ìµœì¢… ì €ì¥ì„ ìˆ˜í–‰
        # raw_pdf_urlì€ raw_s3_keyë¥¼ í†µí•´ êµ¬ì„±í•˜ê±°ë‚˜ APIì—ì„œ ë„˜ê²¨ë°›ì„ ìˆ˜ë„ ìˆì§€ë§Œ,
        # ì—¬ê¸°ì„œëŠ” raw_s3_keyë¥¼ ì•Œê³  ìˆìœ¼ë‹ˆ URLì„ êµ¬ì„±í•˜ê±°ë‚˜ S3 ì„œë¹„ìŠ¤ í•¨ìˆ˜ í™œìš©
        
        # raw_s3_key ì˜ˆ: raw/uuid/filename.pdf
        # s3_service.upload_to_s3ê°€ ë°˜í™˜í•˜ëŠ” í˜•ì‹ì— ë§ì¶°ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ raw_s3_keyë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜, í•„ìš”í•œ URL í¬ë§·ìœ¼ë¡œ ì €ì¥
        
        # upload_to_s3 í•¨ìˆ˜ëŠ” ì „ì²´ URLì„ ë°˜í™˜í•˜ë¯€ë¡œ, API ì„œë²„ì—ì„œ ì—…ë¡œë“œí–ˆì„ ë•Œ ë°›ì€ URLì„ ë„˜ê²¨ë°›ëŠ” ê²Œ ì¢‹ìŒ
        # í•˜ì§€ë§Œ raw_s3_keyë§Œ ë°›ì•„ë„ ì¶©ë¶„í•¨.
        
        raw_pdf_url = f"https://{settings.S3_BUCKET_NAME}.s3.{settings.S3_REGION}.amazonaws.com/{raw_s3_key}"

        meta_info = {
            "category": category,
            "uploaded_by": str(user_id),
            "chunk_count": len(chunks),
            "original_filename": filename,
            "status": "completed"  # ìƒíƒœ í‘œì‹œ
        }
        
        knowledge_doc = KnowledgeDoc(
            id=doc_id,
            filename=filename,
            storage_url=storage_url,
            raw_pdf_url=raw_pdf_url,
            doc_hash=doc_hash,
            file_size=file_size,
            meta_info=meta_info
        )
        
        db.add(knowledge_doc)
        db.commit()
        
        logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: doc_id={doc_id}")

    except Exception as e:
        logger.error(f"íƒœìŠ¤í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ë¡¤ë°±
        db.rollback()
        
        # S3 ì‚­ì œ (Processedë§Œ ì‚­ì œ, RawëŠ” ë‚¨ê¸¸ì§€ ê³ ë¯¼ í•„ìš”í•˜ì§€ë§Œ ì‹¤íŒ¨ ì‹œ ë‹¤ ì§€ìš°ëŠ” ê²Œ ê¹”ë”)
        # Raw íŒŒì¼ì€ íƒœìŠ¤í¬ ì‹œì‘ ì „ API ì„œë²„ê°€ ì˜¬ë¦° ê²ƒ. ì‹¤íŒ¨ ì‹œ ì§€ì›Œì•¼ í•¨.
        try:
            # Raw íŒŒì¼ ì‚­ì œ
            raw_url = f"https://{settings.S3_BUCKET_NAME}.s3.{settings.S3_REGION}.amazonaws.com/{raw_s3_key}"
            delete_from_s3(raw_url)
            
            # Processed íŒŒì¼ ì‚­ì œ
            for url in uploaded_s3_keys:
                delete_from_s3(url)
        except Exception as s3_err:
            logger.error(f"S3 ë¡¤ë°± ì‹¤íŒ¨: {s3_err}")
            
        # Milvus ì‚­ì œ
        if milvus_inserted:
            try:
                collection = get_milvus_collection_safe()
                collection.delete(expr=f'doc_id == "{doc_id_str}"')
                collection.flush()
            except Exception as m_err:
                logger.error(f"Milvus ë¡¤ë°± ì‹¤íŒ¨: {m_err}")
                
        # ì‹¤íŒ¨ ìƒíƒœ DB ê¸°ë¡ ë“±ì´ í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ ìˆ˜í–‰ (ì§€ê¸ˆì€ ìƒëµ)
        
    finally:
        db.close()

