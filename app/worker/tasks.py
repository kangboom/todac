"""
TaskIQ Worker Tasks
ë¬¸ì„œ ì²˜ë¦¬ ë“± ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì •ì˜
"""
import uuid
import logging
import json
import boto3
import gc
from app.core.taskiq import broker
from app.core.database import SessionLocal
from app.models.knowledge import KnowledgeDoc
from app.services.parsers.llama_parse_parser import LlamaParseParser
from app.services.parsers.pymupdf_parser import PyMuPDFParser
from app.services.parsers.docling_parser import DoclingParser
from app.services.chunking_markdown import chunk_markdown_documents
from app.services.markdown_service import cleanup_markdown_with_llm
from app.services.chunking_markdown import chunk_markdown_documents
from app.services.markdown_service import cleanup_markdown_with_llm
from app.services.parser_service import get_parser, get_active_parser
from app.services.s3_service import upload_to_s3, delete_from_s3, generate_storage_paths
from app.agent.tools import get_embedding
from app.core.milvus_schema import MILVUS_COLLECTION_NAME
from app.core.database import get_milvus_client
from app.core.config import settings, get_embeddings
from app.core.milvus_schema import create_milvus_collection

logger = logging.getLogger(__name__)

# S3 í´ë¼ì´ì–¸íŠ¸ (boto3)
s3_client = boto3.client(
    's3',
    aws_access_key_id=settings.AWS_ACCESS_KEY_ID,
    aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,
    region_name=settings.S3_REGION
)

@broker.task
async def process_document_task(
    doc_id_str: str,
    raw_s3_key: str,
    filename: str,
    category: str,
    user_id_str: str,
    file_size: int,
    doc_hash: str
):
    """
    ë°±ê·¸ë¼ìš´ë“œ ë¬¸ì„œ ì²˜ë¦¬ íƒœìŠ¤í¬
    1. S3ì—ì„œ ì›ë³¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    2. ë¬¸ì„œ íŒŒì‹± & Markdown ë³€í™˜
    3. Markdown S3 ì—…ë¡œë“œ
    4. ì²­í‚¹ & ì„ë² ë”©
    5. Milvus ì €ì¥
    6. DB ë©”íƒ€ë°ì´í„° ì €ì¥
    """
    logger.info(f"ğŸš€ ë¬¸ì„œ ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹œì‘: doc_id={doc_id_str}, file={filename}")
    
    db = SessionLocal()
    doc_id = uuid.UUID(doc_id_str)
    user_id = uuid.UUID(user_id_str)
    
    # ë¡¤ë°±ìš© ë¦¬ì†ŒìŠ¤ ì¶”ì 
    uploaded_s3_keys = []
    milvus_inserted = False
    
    try:
        # 1. S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        try:
            response = s3_client.get_object(Bucket=settings.S3_BUCKET_NAME, Key=raw_s3_key)
            content = response['Body'].read()
            logger.info(f"S3 ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(content)} bytes")
        except Exception as e:
            logger.error(f"S3 íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e

        # 2. íŒŒì„œ ì°¾ê¸°
        parser = get_parser(filename)
        if not parser:
            logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {filename}")
            return # ì‹¤íŒ¨ ì²˜ë¦¬ (DB ì—…ë°ì´íŠ¸ ë“± í•„ìš”í•  ìˆ˜ ìˆìŒ)

        # 3. ë¬¸ì„œ íŒŒì‹±
        try:
            documents = parser.parse(content, filename)
            # ë©”ëª¨ë¦¬ ì ˆì•½: íŒŒì‹± ì™„ë£Œ í›„ ì›ë³¸ content ì‚­ì œ
            del content
            gc.collect() 
        except Exception as e:
            logger.error(f"ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise e
            
        # 3-1. Markdown ë³´ì •
        if isinstance(parser, (LlamaParseParser, PyMuPDFParser, DoclingParser)):
            if documents and len(documents) > 0:
                original_text = documents[0].text
                if original_text:
                    cleaned_text = cleanup_markdown_with_llm(original_text, filename)
                    documents[0].text = cleaned_text
                    logger.info(f"Markdown ë³´ì • ì™„ë£Œ: {filename}")

        # 4. í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = chunk_markdown_documents(documents)
        if not chunks:
            raise ValueError("íŒŒì‹±ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # 5. Markdown í…ìŠ¤íŠ¸ S3 ì—…ë¡œë“œ (Processed)
        markdown_text = documents[0].text if documents else ""
        markdown_bytes = markdown_text.encode('utf-8')
        
        storage_paths = generate_storage_paths(doc_id, filename)
        
        # processed_md_keyë¡œ ì—…ë¡œë“œ (raw_s3_keyëŠ” ì´ë¯¸ API ì„œë²„ì—ì„œ ì˜¬ë¦¼)
        storage_url = upload_to_s3(
            content=markdown_bytes,
            s3_key=storage_paths.processed_md_key,
            content_type='text/markdown'
        )
        uploaded_s3_keys.append(storage_url)

        # 6. ì„ë² ë”© ë° Milvus ì €ì¥ (Batch ìµœì í™” ì ìš©)
        try:
            logger.info(f"Milvus ì»¬ë ‰ì…˜ ìƒì„± ì‹œì‘...")
            create_milvus_collection()
            logger.info(f"Milvus ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ...")

            embeddings = get_embeddings()
            client = get_milvus_client()
            
            embedding_texts = []
            prepared_metadata = [] 

            for chunk in chunks:
                # í—¤ë” ì •ë³´ ì¶”ì¶œ
                header_metadata = {
                    k: v for k, v in chunk.metadata.items() 
                    if k.startswith("Header")
                }
                
                # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ êµ¬ì„±
                if header_metadata:
                    sorted_headers = [header_metadata[k] for k in sorted(header_metadata.keys())]
                    header_path = " > ".join(sorted_headers)
                    text_for_embedding = f"{header_path}\n\n{chunk.text}"
                else:
                    text_for_embedding = chunk.text
                
                # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (ë‚˜ì¤‘ì— í•œë°©ì— ë³€í™˜)
                embedding_texts.append(text_for_embedding)
                
                # ë‚˜ì¤‘ì— row ë§Œë“¤ ë•Œ ë§¤ì¹­í•  ì •ë³´ ì €ì¥
                prepared_metadata.append({
                    "chunk": chunk,
                    "headers_json": json.dumps(header_metadata, ensure_ascii=False) if header_metadata else "{}"
                })

            # [Step 2] ë°°ì¹˜ ì„ë² ë”© ì‹¤í–‰ (ê°€ì¥ í° ì„±ëŠ¥ í–¥ìƒ êµ¬ê°„)
            logger.info(f"ì„ë² ë”© ìƒì„± ì‹œì‘ (ì´ {len(embedding_texts)}ê°œ ì²­í¬ Batch ì²˜ë¦¬)...")
            vectors = embeddings.embed_documents(embedding_texts)
            
            # ë©”ëª¨ë¦¬ ì ˆì•½: ì„ë² ë”© ìƒì„± í›„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì‚­ì œ
            del embedding_texts
            gc.collect()
            
            logger.info("ì„ë² ë”© ìƒì„± ì™„ë£Œ")

            # [Step 3] ë°ì´í„° ì¡°ë¦½ ë° Milvus ë°°ì¹˜ ì €ì¥
            batch_size = 100 
            rows = []
            total_count = 0
            
            for i, vector in enumerate(vectors):
                meta = prepared_metadata[i]
                chunk = meta['chunk']
                
                row = {
                    "doc_id": str(doc_id),
                    "chunk_index": chunk.chunk_index,
                    "embedding": vector,
                    "content": chunk.text[:65535],
                    "filename": filename[:255],
                    "category": category[:50],
                    "headers": meta['headers_json'][:2048]
                }
                rows.append(row)
                total_count += 1
                
                # ë°°ì¹˜ ë‹¨ìœ„ ì €ì¥
                if len(rows) >= batch_size:
                    client.insert(
                        collection_name=MILVUS_COLLECTION_NAME,
                        data=rows
                    )
                    logger.info(f"Milvus ë°°ì¹˜ ì €ì¥: {total_count}/{len(chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì¤‘...")
                    rows = []

            # ë‚¨ì€ ë°ì´í„° ì €ì¥
            if rows:
                client.insert(
                    collection_name=MILVUS_COLLECTION_NAME,
                    data=rows
                )
            
            milvus_inserted = True
            logger.info(f"Milvus ì €ì¥ ì™„ë£Œ: ì´ {total_count}ê°œ ì²­í¬")
            
        except Exception as e:
            logger.error(f"Milvus ì €ì¥ ì‹¤íŒ¨: {e}")
            raise e

        # 7. DB ì €ì¥ (KnowledgeDoc)
        raw_pdf_url = f"s3://{settings.S3_BUCKET_NAME}/{raw_s3_key}"

        meta_info = {
            "category": category,
            "uploaded_by": str(user_id),
            "chunk_count": len(chunks),
            "original_filename": filename,
            "status": "completed"  # ìƒíƒœ í‘œì‹œ
        }
        
        knowledge_doc = KnowledgeDoc(
            id=doc_id,
            filename=filename,
            storage_url=storage_url,
            raw_pdf_url=raw_pdf_url,
            doc_hash=doc_hash,
            file_size=file_size,
            meta_info=meta_info
        )
        
        db.add(knowledge_doc)
        db.commit()
        
        logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: doc_id={doc_id}")
        
    except Exception as e:
        logger.error(f"íƒœìŠ¤í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # ë¡¤ë°±
        db.rollback()
        
        # S3 ì‚­ì œ
        try:
            # Raw íŒŒì¼ ì‚­ì œ
            raw_url = f"s3://{settings.S3_BUCKET_NAME}/{raw_s3_key}"
            delete_from_s3(raw_url)
            
            # Processed íŒŒì¼ ì‚­ì œ
            for url in uploaded_s3_keys:
                delete_from_s3(url)
        except Exception as s3_err:
            logger.error(f"S3 ë¡¤ë°± ì‹¤íŒ¨: {s3_err}")
            
        # Milvus ì‚­ì œ
        if milvus_inserted:
            try:
                client = get_milvus_client()
                client.delete(
                    collection_name=MILVUS_COLLECTION_NAME,
                    filter=f'doc_id == "{doc_id_str}"'
                )
            except Exception as m_err:
                logger.error(f"Milvus ë¡¤ë°± ì‹¤íŒ¨: {m_err}")
        
    finally:
        db.close()
        
        # [ë©”ëª¨ë¦¬ ìµœì í™”]
        # 1. ë”¥ëŸ¬ë‹ ëª¨ë¸(Docling) ë“± ë¬´ê±°ìš´ ê°ì²´ì˜ ìºì‹œë¥¼ ë¹„ì›€
        try:
            get_active_parser.cache_clear()
            logger.info("Parser ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ (ë©”ëª¨ë¦¬ ë°˜í™˜)")
        except Exception:
            pass
            
        # 2. ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰
        gc.collect()

